{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Objective of this notebook is to check whether the CNN model can be applied on multivariate tabular dataset or not. If yes then improve the performance of the model by setting proper values for hyperparameters and network architecture.\n",
    "For this purpose, I have taken s&p500 data to predict whether Consumer Price Index will increase or decrease based on the other economic indicators such as __SP500, Dividend, Earnings, Long Interest Rate, Real Price, Real Dividend,  Real Earnings__. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Multivariate CNN Models:__\n",
    "Although traditionally developed for two-dimensional image data, CNNs can be used to model Multivariate time series forecasting problems.\n",
    "\n",
    "Multivariate time series data means data where there is more than one observation for each time step with a temporal ordering and a model is required to learn from the series of past observations to predict the next value in the sequence.\n",
    "\n",
    "This section is divided into two parts; they are:\n",
    "\n",
    "> __A.Data Preparation<br>\n",
    "> B.CNN Model__<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Source: https://datahub.io/core/s-and-p-500__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = pd.read_csv('data_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                      0\n",
       "SP500                     0\n",
       "Dividend                  1\n",
       "Earnings                  4\n",
       "Consumer Price Index      0\n",
       "Long Interest Rate        0\n",
       "Real Price                0\n",
       "Real Dividend             1\n",
       "Real Earnings             4\n",
       "PE10                    120\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Consumer Price Index</th>\n",
       "      <th>Long Interest Rate</th>\n",
       "      <th>Real Price</th>\n",
       "      <th>Real Dividend</th>\n",
       "      <th>Real Earnings</th>\n",
       "      <th>PE10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1871-01-01</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.46</td>\n",
       "      <td>5.32</td>\n",
       "      <td>89.00</td>\n",
       "      <td>5.21</td>\n",
       "      <td>8.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871-02-01</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.84</td>\n",
       "      <td>5.32</td>\n",
       "      <td>87.53</td>\n",
       "      <td>5.06</td>\n",
       "      <td>7.78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1871-03-01</td>\n",
       "      <td>4.61</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>13.03</td>\n",
       "      <td>5.33</td>\n",
       "      <td>88.36</td>\n",
       "      <td>4.98</td>\n",
       "      <td>7.67</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1871-04-01</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.56</td>\n",
       "      <td>5.33</td>\n",
       "      <td>94.29</td>\n",
       "      <td>5.17</td>\n",
       "      <td>7.96</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1871-05-01</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.27</td>\n",
       "      <td>5.33</td>\n",
       "      <td>98.93</td>\n",
       "      <td>5.29</td>\n",
       "      <td>8.14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  SP500  Dividend  Earnings  Consumer Price Index  \\\n",
       "0  1871-01-01   4.44      0.26       0.4                 12.46   \n",
       "1  1871-02-01   4.50      0.26       0.4                 12.84   \n",
       "2  1871-03-01   4.61      0.26       0.4                 13.03   \n",
       "3  1871-04-01   4.74      0.26       0.4                 12.56   \n",
       "4  1871-05-01   4.86      0.26       0.4                 12.27   \n",
       "\n",
       "   Long Interest Rate  Real Price  Real Dividend  Real Earnings  PE10  \n",
       "0                5.32       89.00           5.21           8.02   NaN  \n",
       "1                5.32       87.53           5.06           7.78   NaN  \n",
       "2                5.33       88.36           4.98           7.67   NaN  \n",
       "3                5.33       94.29           5.17           7.96   NaN  \n",
       "4                5.33       98.93           5.29           8.14   NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp[sp.isnull().values.any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Consumer Price Index</th>\n",
       "      <th>Long Interest Rate</th>\n",
       "      <th>Real Price</th>\n",
       "      <th>Real Dividend</th>\n",
       "      <th>Real Earnings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2664.34</td>\n",
       "      <td>48.93</td>\n",
       "      <td>109.88</td>\n",
       "      <td>246.52</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2700.13</td>\n",
       "      <td>49.59</td>\n",
       "      <td>111.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2789.8</td>\n",
       "      <td>49.29</td>\n",
       "      <td>49.29</td>\n",
       "      <td>247.87</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2811.96</td>\n",
       "      <td>49.68</td>\n",
       "      <td>49.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2705.16</td>\n",
       "      <td>49.64</td>\n",
       "      <td>49.64</td>\n",
       "      <td>248.99</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2714.34</td>\n",
       "      <td>49.81</td>\n",
       "      <td>49.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2702.77</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>249.55</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2705.82</td>\n",
       "      <td>50.06</td>\n",
       "      <td>50.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2642.19</td>\n",
       "      <td>2642.19</td>\n",
       "      <td>2642.19</td>\n",
       "      <td>249.84</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2642.19</td>\n",
       "      <td>2642.19</td>\n",
       "      <td>2642.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    SP500 Dividend Earnings Consumer Price Index  \\\n",
       "1763  2017-12-01  2664.34    48.93   109.88               246.52   \n",
       "1764  2018-01-01   2789.8    49.29    49.29               247.87   \n",
       "1765  2018-02-01  2705.16    49.64    49.64               248.99   \n",
       "1766  2018-03-01  2702.77       50       50               249.55   \n",
       "1767  2018-04-01  2642.19  2642.19  2642.19               249.84   \n",
       "\n",
       "     Long Interest Rate Real Price Real Dividend Real Earnings  \n",
       "1763                2.4    2700.13         49.59        111.36  \n",
       "1764               2.58    2811.96         49.68         49.68  \n",
       "1765               2.86    2714.34         49.81         49.81  \n",
       "1766               2.84    2705.82         50.06         50.06  \n",
       "1767                2.8    2642.19       2642.19       2642.19  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Droping the column\n",
    "sp = sp.drop('PE10', axis=1)\n",
    "## fillna with forward fill\n",
    "sp=sp.fillna(method='ffill', axis=1)\n",
    "sp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                    0\n",
       "SP500                   0\n",
       "Dividend                0\n",
       "Earnings                0\n",
       "Consumer Price Index    0\n",
       "Long Interest Rate      0\n",
       "Real Price              0\n",
       "Real Dividend           0\n",
       "Real Earnings           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.84 > 12.46 1\n",
      "13.03 > 12.84 1\n",
      "12.56 < 13.03 0\n",
      "12.27 < 12.56 0\n",
      "12.08 < 12.27 0\n",
      "12.08 < 12.08 0\n",
      "11.89 < 12.08 0\n",
      "12.18 > 11.89 1\n",
      "12.37 > 12.18 1\n",
      "12.37 < 12.37 0\n",
      "12.65 > 12.37 1\n",
      "12.65 < 12.65 0\n",
      "12.65 < 12.65 0\n",
      "12.84 > 12.65 1\n",
      "13.13 > 12.84 1\n",
      "13.13 < 13.13 0\n",
      "13.03 < 13.13 0\n",
      "12.84 < 13.03 0\n",
      "12.94 > 12.84 1\n",
      "13.03 > 12.94 1\n",
      "12.75 < 13.03 0\n",
      "13.13 > 12.75 1\n",
      "12.94 < 13.13 0\n",
      "12.94 < 12.94 0\n",
      "13.23 > 12.94 1\n",
      "13.23 < 13.23 0\n",
      "13.23 < 13.23 0\n",
      "12.94 < 13.23 0\n",
      "12.56 < 12.94 0\n",
      "12.56 < 12.56 0\n",
      "12.56 < 12.56 0\n",
      "12.56 < 12.56 0\n",
      "12.27 < 12.56 0\n",
      "11.89 < 12.27 0\n",
      "12.18 > 11.89 1\n",
      "12.37 > 12.18 1\n",
      "12.37 < 12.37 0\n",
      "12.37 < 12.37 0\n",
      "12.18 < 12.37 0\n",
      "12.08 < 12.18 0\n",
      "11.8 < 12.08 0\n",
      "11.89 > 11.8 1\n",
      "11.8 < 11.89 0\n",
      "11.8 < 11.8 0\n",
      "11.61 < 11.8 0\n",
      "11.51 < 11.61 0\n",
      "11.51 < 11.51 0\n",
      "11.51 < 11.51 0\n",
      "11.51 < 11.51 0\n",
      "11.51 < 11.51 0\n",
      "11.61 > 11.51 1\n",
      "11.32 < 11.61 0\n",
      "11.13 < 11.32 0\n",
      "11.13 < 11.13 0\n",
      "11.23 > 11.13 1\n",
      "11.13 < 11.23 0\n",
      "11.13 < 11.13 0\n",
      "11.04 < 11.13 0\n",
      "10.94 < 11.04 0\n",
      "10.85 < 10.94 0\n",
      "10.85 < 10.85 0\n",
      "10.85 < 10.85 0\n",
      "10.75 < 10.85 0\n",
      "10.37 < 10.75 0\n",
      "10.09 < 10.37 0\n",
      "10.09 < 10.09 0\n",
      "10.18 > 10.09 1\n",
      "10.28 > 10.18 1\n",
      "10.47 > 10.28 1\n",
      "10.56 > 10.47 1\n",
      "10.75 > 10.56 1\n",
      "10.94 > 10.75 1\n",
      "10.66 < 10.94 0\n",
      "10.18 < 10.66 0\n",
      "10.47 > 10.18 1\n",
      "10.66 > 10.47 1\n",
      "10.09 < 10.66 0\n",
      "10.18 > 10.09 1\n",
      "9.8 < 10.18 0\n",
      "9.7 < 9.8 0\n",
      "9.7 < 9.7 0\n",
      "9.51 < 9.7 0\n",
      "9.51 < 9.51 0\n",
      "9.23 < 9.51 0\n",
      "9.13 < 9.23 0\n",
      "8.94 < 9.13 0\n",
      "8.85 < 8.94 0\n",
      "8.56 < 8.85 0\n",
      "8.37 < 8.56 0\n",
      "8.47 > 8.37 1\n",
      "8.56 > 8.47 1\n",
      "8.56 < 8.56 0\n",
      "8.47 < 8.56 0\n",
      "8.37 < 8.47 0\n",
      "8.18 < 8.37 0\n",
      "8.28 > 8.18 1\n",
      "8.37 > 8.28 1\n",
      "8.28 < 8.37 0\n",
      "8.18 < 8.28 0\n",
      "8.18 < 8.18 0\n",
      "8.09 < 8.18 0\n",
      "8.18 > 8.09 1\n",
      "8.18 < 8.18 0\n",
      "8.47 > 8.18 1\n",
      "8.94 > 8.47 1\n",
      "9.42 > 8.94 1\n",
      "9.7 > 9.42 1\n",
      "9.99 > 9.7 1\n",
      "9.99 < 9.99 0\n",
      "10.09 > 9.99 1\n",
      "9.7 < 10.09 0\n",
      "9.42 < 9.7 0\n",
      "9.23 < 9.42 0\n",
      "9.23 < 9.23 0\n",
      "9.23 < 9.23 0\n",
      "9.32 > 9.23 1\n",
      "9.32 < 9.32 0\n",
      "9.42 > 9.32 1\n",
      "9.51 > 9.42 1\n",
      "9.42 < 9.51 0\n",
      "9.51 > 9.42 1\n",
      "9.51 < 9.51 0\n",
      "9.61 > 9.51 1\n",
      "9.51 < 9.61 0\n",
      "9.51 < 9.51 0\n",
      "9.61 > 9.51 1\n",
      "9.8 > 9.61 1\n",
      "10.18 > 9.8 1\n",
      "10.28 > 10.18 1\n",
      "10.18 < 10.28 0\n",
      "10.18 < 10.18 0\n",
      "10.18 < 10.18 0\n",
      "10.28 > 10.18 1\n",
      "10.28 < 10.28 0\n",
      "10.37 > 10.28 1\n",
      "10.47 > 10.37 1\n",
      "10.56 > 10.47 1\n",
      "10.47 < 10.56 0\n",
      "10.56 > 10.47 1\n",
      "10.28 < 10.56 0\n",
      "10.18 < 10.28 0\n",
      "10.09 < 10.18 0\n",
      "9.99 < 10.09 0\n",
      "9.99 < 9.99 0\n",
      "10.09 > 9.99 1\n",
      "9.99 < 10.09 0\n",
      "9.9 < 9.99 0\n",
      "9.8 < 9.9 0\n",
      "9.51 < 9.8 0\n",
      "9.32 < 9.51 0\n",
      "9.32 < 9.32 0\n",
      "9.23 < 9.32 0\n",
      "9.23 < 9.23 0\n",
      "9.13 < 9.23 0\n",
      "9.23 > 9.13 1\n",
      "9.23 < 9.23 0\n",
      "9.23 < 9.23 0\n",
      "9.23 < 9.23 0\n",
      "9.04 < 9.23 0\n",
      "8.85 < 9.04 0\n",
      "8.85 < 8.85 0\n",
      "8.75 < 8.85 0\n",
      "8.75 < 8.75 0\n",
      "8.66 < 8.75 0\n",
      "8.56 < 8.66 0\n",
      "8.37 < 8.56 0\n",
      "8.28 < 8.37 0\n",
      "8.28 < 8.28 0\n",
      "8.37 > 8.28 1\n",
      "8.18 < 8.37 0\n",
      "8.28 > 8.18 1\n",
      "8.09 < 8.28 0\n",
      "7.9 < 8.09 0\n",
      "7.99 > 7.9 1\n",
      "7.99 < 7.99 0\n",
      "7.9 < 7.99 0\n",
      "7.9 < 7.9 0\n",
      "7.99 > 7.9 1\n",
      "8.18 > 7.99 1\n",
      "7.99 < 8.18 0\n",
      "7.99 < 7.99 0\n",
      "7.9 < 7.99 0\n",
      "7.8 < 7.9 0\n",
      "7.61 < 7.8 0\n",
      "7.52 < 7.61 0\n",
      "7.61 > 7.52 1\n",
      "7.71 > 7.61 1\n",
      "7.71 < 7.71 0\n",
      "7.71 < 7.71 0\n",
      "7.71 < 7.71 0\n",
      "7.8 > 7.71 1\n",
      "7.99 > 7.8 1\n",
      "8.09 > 7.99 1\n",
      "8.09 < 8.09 0\n",
      "8.09 < 8.09 0\n",
      "8.09 < 8.09 0\n",
      "7.99 < 8.09 0\n",
      "7.9 < 7.99 0\n",
      "7.99 > 7.9 1\n",
      "7.9 < 7.99 0\n",
      "7.99 > 7.9 1\n",
      "8.09 > 7.99 1\n",
      "8.28 > 8.09 1\n",
      "8.37 > 8.28 1\n",
      "8.28 < 8.37 0\n",
      "8.28 < 8.28 0\n",
      "8.18 < 8.28 0\n",
      "8.09 < 8.18 0\n",
      "7.99 < 8.09 0\n",
      "8.09 > 7.99 1\n",
      "8.09 < 8.09 0\n",
      "8.09 < 8.09 0\n",
      "8.18 > 8.09 1\n",
      "8.28 > 8.18 1\n",
      "8.28 < 8.28 0\n",
      "7.99 < 8.28 0\n",
      "7.9 < 7.99 0\n",
      "7.8 < 7.9 0\n",
      "7.8 < 7.8 0\n",
      "7.61 < 7.8 0\n",
      "7.61 < 7.61 0\n",
      "7.61 < 7.61 0\n",
      "7.61 < 7.61 0\n",
      "7.71 > 7.61 1\n",
      "7.71 < 7.71 0\n",
      "7.71 < 7.71 0\n",
      "7.8 > 7.71 1\n",
      "7.61 < 7.8 0\n",
      "7.61 < 7.61 0\n",
      "7.61 < 7.61 0\n",
      "7.61 < 7.61 0\n",
      "7.71 > 7.61 1\n",
      "7.71 < 7.71 0\n",
      "7.71 < 7.71 0\n",
      "7.99 > 7.71 1\n",
      "8.09 > 7.99 1\n",
      "8.09 < 8.09 0\n",
      "7.9 < 8.09 0\n",
      "7.9 < 7.9 0\n",
      "7.8 < 7.9 0\n",
      "7.9 > 7.8 1\n",
      "7.99 > 7.9 1\n",
      "8.09 > 7.99 1\n",
      "7.99 < 8.09 0\n",
      "7.8 < 7.99 0\n",
      "7.71 < 7.8 0\n",
      "7.71 < 7.71 0\n",
      "7.61 < 7.71 0\n",
      "7.61 < 7.61 0\n",
      "7.52 < 7.61 0\n",
      "7.52 < 7.52 0\n",
      "7.33 < 7.52 0\n",
      "7.33 < 7.33 0\n",
      "7.14 < 7.33 0\n",
      "7.04 < 7.14 0\n",
      "7.04 < 7.04 0\n",
      "7.04 < 7.04 0\n",
      "7.23 > 7.04 1\n",
      "7.33 > 7.23 1\n",
      "7.33 < 7.33 0\n",
      "7.33 < 7.33 0\n",
      "7.52 > 7.33 1\n",
      "7.61 > 7.52 1\n",
      "7.9 > 7.61 1\n",
      "7.99 > 7.9 1\n",
      "7.8 < 7.99 0\n",
      "7.71 < 7.8 0\n",
      "7.61 < 7.71 0\n",
      "7.42 < 7.61 0\n",
      "7.23 < 7.42 0\n",
      "6.95 < 7.23 0\n",
      "7.23 > 6.95 1\n",
      "7.33 > 7.23 1\n",
      "7.14 < 7.33 0\n",
      "7.04 < 7.14 0\n",
      "6.85 < 7.04 0\n",
      "6.76 < 6.85 0\n",
      "6.57 < 6.76 0\n",
      "6.57 < 6.57 0\n",
      "6.57 < 6.57 0\n",
      "6.57 < 6.57 0\n",
      "6.57 < 6.57 0\n",
      "6.76 > 6.57 1\n",
      "6.85 > 6.76 1\n",
      "6.66 < 6.85 0\n",
      "6.66 < 6.66 0\n",
      "6.57 < 6.66 0\n",
      "6.57 < 6.57 0\n",
      "6.57 < 6.57 0\n",
      "6.57 < 6.57 0\n",
      "6.85 > 6.57 1\n",
      "6.95 > 6.85 1\n",
      "7.04 > 6.95 1\n",
      "6.95 < 7.04 0\n",
      "6.85 < 6.95 0\n",
      "6.85 < 6.85 0\n",
      "6.85 < 6.85 0\n",
      "6.85 < 6.85 0\n",
      "6.76 < 6.85 0\n",
      "6.66 < 6.76 0\n",
      "6.57 < 6.66 0\n",
      "6.57 < 6.57 0\n",
      "6.47 < 6.57 0\n",
      "6.37 < 6.47 0\n",
      "6.28 < 6.37 0\n",
      "6.28 < 6.28 0\n",
      "6.28 < 6.28 0\n",
      "6.28 < 6.28 0\n",
      "6.47 > 6.28 1\n",
      "6.66 > 6.47 1\n",
      "6.66 < 6.66 0\n",
      "6.47 < 6.66 0\n",
      "6.47 < 6.47 0\n",
      "6.47 < 6.47 0\n",
      "6.37 < 6.47 0\n",
      "6.28 < 6.37 0\n",
      "6.28 < 6.28 0\n",
      "6.28 < 6.28 0\n",
      "6.57 > 6.28 1\n",
      "6.76 > 6.57 1\n",
      "6.66 < 6.76 0\n",
      "6.66 < 6.66 0\n",
      "6.66 < 6.66 0\n",
      "6.66 < 6.66 0\n",
      "6.76 > 6.66 1\n",
      "6.76 < 6.76 0\n",
      "6.76 < 6.76 0\n",
      "7.23 > 6.76 1\n",
      "6.76 < 7.23 0\n",
      "6.66 < 6.76 0\n",
      "6.66 < 6.66 0\n",
      "6.66 < 6.66 0\n",
      "6.66 < 6.66 0\n",
      "6.66 < 6.66 0\n",
      "6.76 > 6.66 1\n",
      "6.76 < 6.76 0\n",
      "6.95 > 6.76 1\n",
      "6.95 < 6.95 0\n",
      "7.04 > 6.95 1\n",
      "7.04 < 7.04 0\n",
      "7.14 > 7.04 1\n",
      "7.23 > 7.14 1\n",
      "7.33 > 7.23 1\n",
      "7.61 > 7.33 1\n",
      "7.71 > 7.61 1\n",
      "7.8 > 7.71 1\n",
      "7.9 > 7.8 1\n",
      "7.9 < 7.9 0\n",
      "7.99 > 7.9 1\n",
      "7.99 < 7.99 0\n",
      "7.99 < 7.99 0\n",
      "7.8 < 7.99 0\n",
      "7.71 < 7.8 0\n",
      "7.8 > 7.71 1\n",
      "7.71 < 7.8 0\n",
      "7.8 > 7.71 1\n",
      "7.71 < 7.8 0\n",
      "7.71 < 7.71 0\n",
      "7.61 < 7.71 0\n",
      "7.71 > 7.61 1\n",
      "7.61 < 7.71 0\n",
      "7.61 < 7.61 0\n",
      "7.52 < 7.61 0\n",
      "7.52 < 7.52 0\n",
      "7.52 < 7.52 0\n",
      "7.61 > 7.52 1\n",
      "7.71 > 7.61 1\n",
      "7.8 > 7.71 1\n",
      "7.8 < 7.8 0\n",
      "7.9 > 7.8 1\n",
      "7.99 > 7.9 1\n",
      "7.9 < 7.99 0\n",
      "7.9 < 7.9 0\n",
      "7.9 < 7.9 0\n",
      "7.99 > 7.9 1\n",
      "8.09 > 7.99 1\n",
      "8.18 > 8.09 1\n",
      "8.18 < 8.18 0\n",
      "8.09 < 8.18 0\n",
      "8.18 > 8.09 1\n",
      "8.75 > 8.18 1\n",
      "8.47 < 8.75 0\n",
      "8.56 > 8.47 1\n",
      "8.66 > 8.56 1\n",
      "8.66 < 8.66 0\n",
      "8.37 < 8.66 0\n",
      "8.37 < 8.37 0\n",
      "8.18 < 8.37 0\n",
      "8.18 < 8.18 0\n",
      "8.18 < 8.18 0\n",
      "8.18 < 8.18 0\n",
      "8.28 > 8.18 1\n",
      "8.18 < 8.28 0\n",
      "8.09 < 8.18 0\n",
      "8.09 < 8.09 0\n",
      "8.28 > 8.09 1\n",
      "8.47 > 8.28 1\n",
      "8.37 < 8.47 0\n",
      "8.28 < 8.37 0\n",
      "8.09 < 8.28 0\n",
      "8.09 < 8.09 0\n",
      "8.09 < 8.09 0\n",
      "8.18 > 8.09 1\n",
      "8.28 > 8.18 1\n",
      "8.28 < 8.28 0\n",
      "8.47 > 8.28 1\n",
      "8.47 < 8.47 0\n",
      "8.47 < 8.47 0\n",
      "8.47 < 8.47 0\n",
      "8.37 < 8.47 0\n",
      "8.37 < 8.37 0\n",
      "8.28 < 8.37 0\n",
      "8.28 < 8.28 0\n",
      "8.28 < 8.28 0\n",
      "8.37 > 8.28 1\n",
      "8.28 < 8.37 0\n",
      "8.28 < 8.28 0\n",
      "8.37 > 8.28 1\n",
      "8.47 > 8.37 1\n",
      "8.47 < 8.47 0\n",
      "8.47 < 8.47 0\n",
      "8.47 < 8.47 0\n",
      "8.47 < 8.47 0\n",
      "8.56 > 8.47 1\n",
      "8.56 < 8.56 0\n",
      "8.28 < 8.56 0\n",
      "8.47 > 8.28 1\n",
      "8.56 > 8.47 1\n",
      "8.75 > 8.56 1\n",
      "8.85 > 8.75 1\n",
      "8.94 > 8.85 1\n",
      "8.85 < 8.94 0\n",
      "9.04 > 8.85 1\n",
      "8.94 < 9.04 0\n",
      "8.94 < 8.94 0\n",
      "9.13 > 8.94 1\n",
      "9.23 > 9.13 1\n",
      "9.23 < 9.23 0\n",
      "9.23 < 9.23 0\n",
      "9.23 < 9.23 0\n",
      "9.32 > 9.23 1\n",
      "8.94 < 9.32 0\n",
      "8.75 < 8.94 0\n",
      "8.66 < 8.75 0\n",
      "8.56 < 8.66 0\n",
      "8.56 < 8.56 0\n",
      "8.66 > 8.56 1\n",
      "8.66 < 8.66 0\n",
      "8.66 < 8.66 0\n",
      "8.75 > 8.66 1\n",
      "8.75 < 8.75 0\n",
      "8.75 < 8.75 0\n",
      "8.85 > 8.75 1\n",
      "8.94 > 8.85 1\n",
      "9.04 > 8.94 1\n",
      "8.94 < 9.04 0\n",
      "9.04 > 8.94 1\n",
      "9.04 < 9.04 0\n",
      "9.23 > 9.04 1\n",
      "9.32 > 9.23 1\n",
      "9.42 > 9.32 1\n",
      "9.42 < 9.42 0\n",
      "9.51 > 9.42 1\n",
      "9.61 > 9.51 1\n",
      "9.8 > 9.61 1\n",
      "9.9 > 9.8 1\n",
      "9.99 > 9.9 1\n",
      "9.9 < 9.99 0\n",
      "9.9 < 9.9 0\n",
      "10.09 > 9.9 1\n",
      "10.18 > 10.09 1\n",
      "9.99 < 10.18 0\n",
      "9.9 < 9.99 0\n",
      "9.9 < 9.9 0\n",
      "9.8 < 9.9 0\n",
      "9.7 < 9.8 0\n",
      "9.42 < 9.7 0\n",
      "9.23 < 9.42 0\n",
      "9.23 < 9.23 0\n",
      "9.23 < 9.23 0\n",
      "8.94 < 9.23 0\n",
      "9.04 > 8.94 1\n",
      "8.75 < 9.04 0\n",
      "8.75 < 8.75 0\n",
      "8.75 < 8.75 0\n",
      "8.85 > 8.75 1\n",
      "9.13 > 8.85 1\n",
      "9.23 > 9.13 1\n",
      "9.23 < 9.23 0\n",
      "9.13 < 9.23 0\n",
      "9.04 < 9.13 0\n",
      "9.13 > 9.04 1\n",
      "9.23 > 9.13 1\n",
      "9.42 > 9.23 1\n",
      "9.7 > 9.42 1\n",
      "9.7 < 9.7 0\n",
      "9.61 < 9.7 0\n",
      "9.61 < 9.61 0\n",
      "9.7 > 9.61 1\n",
      "9.8 > 9.7 1\n",
      "9.8 < 9.8 0\n",
      "9.8 < 9.8 0\n",
      "9.7 < 9.8 0\n",
      "9.8 > 9.7 1\n",
      "9.8 < 9.8 0\n",
      "9.8 < 9.8 0\n",
      "9.8 < 9.8 0\n",
      "9.7 < 9.8 0\n",
      "9.8 > 9.7 1\n",
      "9.9 > 9.8 1\n",
      "9.9 < 9.9 0\n",
      "10.0 > 9.9 1\n",
      "10.0 < 10.0 0\n",
      "10.1 > 10.0 1\n",
      "10.0 < 10.1 0\n",
      "10.0 < 10.0 0\n",
      "9.9 < 10.0 0\n",
      "9.9 < 9.9 0\n",
      "9.8 < 9.9 0\n",
      "9.9 > 9.8 1\n",
      "9.9 < 9.9 0\n",
      "10.0 > 9.9 1\n",
      "10.2 > 10.0 1\n",
      "10.2 < 10.2 0\n",
      "10.1 < 10.2 0\n",
      "10.2 > 10.1 1\n",
      "10.1 < 10.2 0\n",
      "10.1 < 10.1 0\n",
      "10.0 < 10.1 0\n",
      "9.9 < 10.0 0\n",
      "10.0 > 9.9 1\n",
      "10.1 > 10.0 1\n",
      "10.1 < 10.1 0\n",
      "10.1 < 10.1 0\n",
      "10.1 < 10.1 0\n",
      "10.1 < 10.1 0\n",
      "10.2 > 10.1 1\n",
      "10.3 > 10.2 1\n",
      "10.3 < 10.3 0\n",
      "10.4 > 10.3 1\n",
      "10.4 < 10.4 0\n",
      "10.5 > 10.4 1\n",
      "10.6 > 10.5 1\n",
      "10.7 > 10.6 1\n",
      "10.8 > 10.7 1\n",
      "10.8 < 10.8 0\n",
      "10.9 > 10.8 1\n",
      "11.1 > 10.9 1\n",
      "11.3 > 11.1 1\n",
      "11.5 > 11.3 1\n",
      "11.6 > 11.5 1\n",
      "11.7 > 11.6 1\n",
      "12.0 > 11.7 1\n",
      "12.0 < 12.0 0\n",
      "12.6 > 12.0 1\n",
      "12.8 > 12.6 1\n",
      "13.0 > 12.8 1\n",
      "12.8 < 13.0 0\n",
      "13.0 > 12.8 1\n",
      "13.3 > 13.0 1\n",
      "13.5 > 13.3 1\n",
      "13.5 < 13.5 0\n",
      "13.7 > 13.5 1\n",
      "14.0 > 13.7 1\n",
      "14.1 > 14.0 1\n",
      "14.0 < 14.1 0\n",
      "14.2 > 14.0 1\n",
      "14.5 > 14.2 1\n",
      "14.7 > 14.5 1\n",
      "15.1 > 14.7 1\n",
      "15.4 > 15.1 1\n",
      "15.7 > 15.4 1\n",
      "16.0 > 15.7 1\n",
      "16.3 > 16.0 1\n",
      "16.5 > 16.3 1\n",
      "16.5 < 16.5 0\n",
      "16.2 < 16.5 0\n",
      "16.4 > 16.2 1\n",
      "16.7 > 16.4 1\n",
      "16.9 > 16.7 1\n",
      "16.9 < 16.9 0\n",
      "17.4 > 16.9 1\n",
      "17.7 > 17.4 1\n",
      "17.8 > 17.7 1\n",
      "18.1 > 17.8 1\n",
      "18.5 > 18.1 1\n",
      "18.9 > 18.5 1\n",
      "19.3 > 18.9 1\n",
      "19.5 > 19.3 1\n",
      "19.7 > 19.5 1\n",
      "20.3 > 19.7 1\n",
      "20.6 > 20.3 1\n",
      "20.9 > 20.6 1\n",
      "20.8 < 20.9 0\n",
      "20.3 < 20.8 0\n",
      "20.0 < 20.3 0\n",
      "19.9 < 20.0 0\n",
      "19.8 < 19.9 0\n",
      "19.4 < 19.8 0\n",
      "19.0 < 19.4 0\n",
      "18.4 < 19.0 0\n",
      "18.3 < 18.4 0\n",
      "18.1 < 18.3 0\n",
      "17.7 < 18.1 0\n",
      "17.6 < 17.7 0\n",
      "17.7 > 17.6 1\n",
      "17.7 < 17.7 0\n",
      "17.5 < 17.7 0\n",
      "17.5 < 17.5 0\n",
      "17.4 < 17.5 0\n",
      "17.3 < 17.4 0\n",
      "16.9 < 17.3 0\n",
      "16.9 < 16.9 0\n",
      "16.7 < 16.9 0\n",
      "16.7 < 16.7 0\n",
      "16.7 < 16.7 0\n",
      "16.7 < 16.7 0\n",
      "16.8 > 16.7 1\n",
      "16.6 < 16.8 0\n",
      "16.6 < 16.6 0\n",
      "16.7 > 16.6 1\n",
      "16.8 > 16.7 1\n",
      "16.9 > 16.8 1\n",
      "16.8 < 16.9 0\n",
      "16.8 < 16.8 0\n",
      "16.8 < 16.8 0\n",
      "16.9 > 16.8 1\n",
      "16.9 < 16.9 0\n",
      "17.0 > 16.9 1\n",
      "17.2 > 17.0 1\n",
      "17.1 < 17.2 0\n",
      "17.2 > 17.1 1\n",
      "17.3 > 17.2 1\n",
      "17.3 < 17.3 0\n",
      "17.3 < 17.3 0\n",
      "17.3 < 17.3 0\n",
      "17.2 < 17.3 0\n",
      "17.1 < 17.2 0\n",
      "17.0 < 17.1 0\n",
      "17.0 < 17.0 0\n",
      "17.0 < 17.0 0\n",
      "17.1 > 17.0 1\n",
      "17.0 < 17.1 0\n",
      "17.1 > 17.0 1\n",
      "17.2 > 17.1 1\n",
      "17.2 < 17.2 0\n",
      "17.3 > 17.2 1\n",
      "17.3 < 17.3 0\n",
      "17.2 < 17.3 0\n",
      "17.3 > 17.2 1\n",
      "17.2 < 17.3 0\n",
      "17.3 > 17.2 1\n",
      "17.5 > 17.3 1\n",
      "17.7 > 17.5 1\n",
      "17.7 < 17.7 0\n",
      "17.7 < 17.7 0\n",
      "17.7 < 17.7 0\n",
      "18.0 > 17.7 1\n",
      "17.9 < 18.0 0\n",
      "17.9 < 17.9 0\n",
      "17.9 < 17.9 0\n",
      "17.8 < 17.9 0\n",
      "17.9 > 17.8 1\n",
      "17.8 < 17.9 0\n",
      "17.7 < 17.8 0\n",
      "17.5 < 17.7 0\n",
      "17.4 < 17.5 0\n",
      "17.5 > 17.4 1\n",
      "17.6 > 17.5 1\n",
      "17.7 > 17.6 1\n",
      "17.7 < 17.7 0\n",
      "17.5 < 17.7 0\n",
      "17.4 < 17.5 0\n",
      "17.3 < 17.4 0\n",
      "17.3 < 17.3 0\n",
      "17.4 > 17.3 1\n",
      "17.6 > 17.4 1\n",
      "17.3 < 17.6 0\n",
      "17.2 < 17.3 0\n",
      "17.3 > 17.2 1\n",
      "17.4 > 17.3 1\n",
      "17.3 < 17.4 0\n",
      "17.3 < 17.3 0\n",
      "17.3 < 17.3 0\n",
      "17.1 < 17.3 0\n",
      "17.1 < 17.1 0\n",
      "17.1 < 17.1 0\n",
      "17.2 > 17.1 1\n",
      "17.1 < 17.2 0\n",
      "17.1 < 17.1 0\n",
      "17.1 < 17.1 0\n",
      "17.3 > 17.1 1\n",
      "17.2 < 17.3 0\n",
      "17.2 < 17.2 0\n",
      "17.1 < 17.2 0\n",
      "17.1 < 17.1 0\n",
      "17.1 < 17.1 0\n",
      "17.0 < 17.1 0\n",
      "16.9 < 17.0 0\n",
      "17.0 > 16.9 1\n",
      "17.1 > 17.0 1\n",
      "17.3 > 17.1 1\n",
      "17.3 < 17.3 0\n",
      "17.3 < 17.3 0\n",
      "17.3 < 17.3 0\n",
      "17.3 < 17.3 0\n",
      "17.2 < 17.3 0\n",
      "17.1 < 17.2 0\n",
      "17.0 < 17.1 0\n",
      "16.9 < 17.0 0\n",
      "17.0 > 16.9 1\n",
      "16.9 < 17.0 0\n",
      "16.8 < 16.9 0\n",
      "16.6 < 16.8 0\n",
      "16.5 < 16.6 0\n",
      "16.6 > 16.5 1\n",
      "16.5 < 16.6 0\n",
      "16.4 < 16.5 0\n",
      "16.1 < 16.4 0\n",
      "15.9 < 16.1 0\n",
      "15.7 < 15.9 0\n",
      "15.6 < 15.7 0\n",
      "15.5 < 15.6 0\n",
      "15.3 < 15.5 0\n",
      "15.1 < 15.3 0\n",
      "15.1 < 15.1 0\n",
      "15.1 < 15.1 0\n",
      "15.0 < 15.1 0\n",
      "14.9 < 15.0 0\n",
      "14.7 < 14.9 0\n",
      "14.6 < 14.7 0\n",
      "14.3 < 14.6 0\n",
      "14.1 < 14.3 0\n",
      "14.0 < 14.1 0\n",
      "13.9 < 14.0 0\n",
      "13.7 < 13.9 0\n",
      "13.6 < 13.7 0\n",
      "13.6 < 13.6 0\n",
      "13.5 < 13.6 0\n",
      "13.4 < 13.5 0\n",
      "13.3 < 13.4 0\n",
      "13.2 < 13.3 0\n",
      "13.1 < 13.2 0\n",
      "12.9 < 13.1 0\n",
      "12.7 < 12.9 0\n",
      "12.6 < 12.7 0\n",
      "12.6 < 12.6 0\n",
      "12.6 < 12.6 0\n",
      "12.7 > 12.6 1\n",
      "13.1 > 12.7 1\n",
      "13.2 > 13.1 1\n",
      "13.2 < 13.2 0\n",
      "13.2 < 13.2 0\n",
      "13.2 < 13.2 0\n",
      "13.2 < 13.2 0\n",
      "13.2 < 13.2 0\n",
      "13.3 > 13.2 1\n",
      "13.3 < 13.3 0\n",
      "13.3 < 13.3 0\n",
      "13.3 < 13.3 0\n",
      "13.4 > 13.3 1\n",
      "13.4 < 13.4 0\n",
      "13.4 < 13.4 0\n",
      "13.6 > 13.4 1\n",
      "13.5 < 13.6 0\n",
      "13.5 < 13.5 0\n",
      "13.4 < 13.5 0\n",
      "13.6 > 13.4 1\n",
      "13.7 > 13.6 1\n",
      "13.7 < 13.7 0\n",
      "13.8 > 13.7 1\n",
      "13.8 < 13.8 0\n",
      "13.7 < 13.8 0\n",
      "13.7 < 13.7 0\n",
      "13.7 < 13.7 0\n",
      "13.7 < 13.7 0\n",
      "13.7 < 13.7 0\n",
      "13.8 > 13.7 1\n",
      "13.8 < 13.8 0\n",
      "13.8 < 13.8 0\n",
      "13.8 < 13.8 0\n",
      "13.7 < 13.8 0\n",
      "13.7 < 13.7 0\n",
      "13.7 < 13.7 0\n",
      "13.8 > 13.7 1\n",
      "13.9 > 13.8 1\n",
      "14.0 > 13.9 1\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "14.1 > 14.0 1\n",
      "14.1 < 14.1 0\n",
      "14.2 > 14.1 1\n",
      "14.3 > 14.2 1\n",
      "14.4 > 14.3 1\n",
      "14.4 < 14.4 0\n",
      "14.5 > 14.4 1\n",
      "14.5 < 14.5 0\n",
      "14.6 > 14.5 1\n",
      "14.6 < 14.6 0\n",
      "14.5 < 14.6 0\n",
      "14.4 < 14.5 0\n",
      "14.2 < 14.4 0\n",
      "14.1 < 14.2 0\n",
      "14.1 < 14.1 0\n",
      "14.2 > 14.1 1\n",
      "14.1 < 14.2 0\n",
      "14.1 < 14.1 0\n",
      "14.1 < 14.1 0\n",
      "14.1 < 14.1 0\n",
      "14.1 < 14.1 0\n",
      "14.0 < 14.1 0\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "13.9 < 14.0 0\n",
      "13.9 < 13.9 0\n",
      "13.8 < 13.9 0\n",
      "13.8 < 13.8 0\n",
      "13.8 < 13.8 0\n",
      "13.8 < 13.8 0\n",
      "13.8 < 13.8 0\n",
      "14.1 > 13.8 1\n",
      "14.0 < 14.1 0\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "13.9 < 14.0 0\n",
      "14.0 > 13.9 1\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "14.1 > 14.0 1\n",
      "14.0 < 14.1 0\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "14.0 < 14.0 0\n",
      "14.1 > 14.0 1\n",
      "14.1 < 14.1 0\n",
      "14.1 < 14.1 0\n",
      "14.2 > 14.1 1\n",
      "14.3 > 14.2 1\n",
      "14.4 > 14.3 1\n",
      "14.7 > 14.4 1\n",
      "14.7 < 14.7 0\n",
      "14.9 > 14.7 1\n",
      "15.1 > 14.9 1\n",
      "15.3 > 15.1 1\n",
      "15.4 > 15.3 1\n",
      "15.5 > 15.4 1\n",
      "15.7 > 15.5 1\n",
      "15.8 > 15.7 1\n",
      "16.0 > 15.8 1\n",
      "16.1 > 16.0 1\n",
      "16.3 > 16.1 1\n",
      "16.3 < 16.3 0\n",
      "16.4 > 16.3 1\n",
      "16.5 > 16.4 1\n",
      "16.5 < 16.5 0\n",
      "16.7 > 16.5 1\n",
      "16.8 > 16.7 1\n",
      "16.9 > 16.8 1\n",
      "16.9 < 16.9 0\n",
      "16.9 < 16.9 0\n",
      "17.2 > 16.9 1\n",
      "17.4 > 17.2 1\n",
      "17.5 > 17.4 1\n",
      "17.5 < 17.5 0\n",
      "17.4 < 17.5 0\n",
      "17.3 < 17.4 0\n",
      "17.4 > 17.3 1\n",
      "17.4 < 17.4 0\n",
      "17.4 < 17.4 0\n",
      "17.4 < 17.4 0\n",
      "17.4 < 17.4 0\n",
      "17.4 < 17.4 0\n",
      "17.4 < 17.4 0\n",
      "17.5 > 17.4 1\n",
      "17.5 < 17.5 0\n",
      "17.6 > 17.5 1\n",
      "17.7 > 17.6 1\n",
      "17.7 < 17.7 0\n",
      "17.7 < 17.7 0\n",
      "17.7 < 17.7 0\n",
      "17.7 < 17.7 0\n",
      "17.8 > 17.7 1\n",
      "17.8 < 17.8 0\n",
      "17.8 < 17.8 0\n",
      "17.8 < 17.8 0\n",
      "17.8 < 17.8 0\n",
      "17.9 > 17.8 1\n",
      "18.1 > 17.9 1\n",
      "18.1 < 18.1 0\n",
      "18.1 < 18.1 0\n",
      "18.1 < 18.1 0\n",
      "18.1 < 18.1 0\n",
      "18.1 < 18.1 0\n",
      "18.2 > 18.1 1\n",
      "18.2 < 18.2 0\n",
      "18.1 < 18.2 0\n",
      "18.3 > 18.1 1\n",
      "18.4 > 18.3 1\n",
      "18.5 > 18.4 1\n",
      "18.7 > 18.5 1\n",
      "19.8 > 18.7 1\n",
      "20.2 > 19.8 1\n",
      "20.4 > 20.2 1\n",
      "20.8 > 20.4 1\n",
      "21.3 > 20.8 1\n",
      "21.5 > 21.3 1\n",
      "21.5 < 21.5 0\n",
      "21.5 < 21.5 0\n",
      "21.9 > 21.5 1\n",
      "21.9 < 21.9 0\n",
      "21.9 < 21.9 0\n",
      "22.0 > 21.9 1\n",
      "22.2 > 22.0 1\n",
      "22.5 > 22.2 1\n",
      "23.0 > 22.5 1\n",
      "23.0 < 23.0 0\n",
      "23.1 > 23.0 1\n",
      "23.4 > 23.1 1\n",
      "23.7 > 23.4 1\n",
      "23.5 < 23.7 0\n",
      "23.4 < 23.5 0\n",
      "23.8 > 23.4 1\n",
      "23.9 > 23.8 1\n",
      "24.1 > 23.9 1\n",
      "24.4 > 24.1 1\n",
      "24.5 > 24.4 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.5 < 24.5 0\n",
      "24.4 < 24.5 0\n",
      "24.2 < 24.4 0\n",
      "24.1 < 24.2 0\n",
      "24.0 < 24.1 0\n",
      "23.8 < 24.0 0\n",
      "23.8 < 23.8 0\n",
      "23.9 > 23.8 1\n",
      "23.8 < 23.9 0\n",
      "23.9 > 23.8 1\n",
      "23.7 < 23.9 0\n",
      "23.8 > 23.7 1\n",
      "23.9 > 23.8 1\n",
      "23.7 < 23.9 0\n",
      "23.8 > 23.7 1\n",
      "23.6 < 23.8 0\n",
      "23.5 < 23.6 0\n",
      "23.5 < 23.5 0\n",
      "23.6 > 23.5 1\n",
      "23.6 < 23.6 0\n",
      "23.7 > 23.6 1\n",
      "23.8 > 23.7 1\n",
      "24.1 > 23.8 1\n",
      "24.3 > 24.1 1\n",
      "24.4 > 24.3 1\n",
      "24.6 > 24.4 1\n",
      "24.7 > 24.6 1\n",
      "25.0 > 24.7 1\n",
      "25.4 > 25.0 1\n",
      "25.7 > 25.4 1\n",
      "25.8 > 25.7 1\n",
      "25.8 < 25.8 0\n",
      "25.9 > 25.8 1\n",
      "25.9 < 25.9 0\n",
      "25.9 < 25.9 0\n",
      "25.9 < 25.9 0\n",
      "26.1 > 25.9 1\n",
      "26.2 > 26.1 1\n",
      "26.4 > 26.2 1\n",
      "26.5 > 26.4 1\n",
      "26.5 < 26.5 0\n",
      "26.3 < 26.5 0\n",
      "26.3 < 26.3 0\n",
      "26.4 > 26.3 1\n",
      "26.4 < 26.4 0\n",
      "26.5 > 26.4 1\n",
      "26.7 > 26.5 1\n",
      "26.7 < 26.7 0\n",
      "26.7 < 26.7 0\n",
      "26.7 < 26.7 0\n",
      "26.7 < 26.7 0\n",
      "26.7 < 26.7 0\n",
      "26.6 < 26.7 0\n",
      "26.5 < 26.6 0\n",
      "26.6 > 26.5 1\n",
      "26.6 < 26.6 0\n",
      "26.7 > 26.6 1\n",
      "26.8 > 26.7 1\n",
      "26.8 < 26.8 0\n",
      "26.9 > 26.8 1\n",
      "26.9 < 26.9 0\n",
      "27.0 > 26.9 1\n",
      "26.9 < 27.0 0\n",
      "26.9 < 26.9 0\n",
      "26.9 < 26.9 0\n",
      "26.9 < 26.9 0\n",
      "26.9 < 26.9 0\n",
      "26.8 < 26.9 0\n",
      "26.9 > 26.8 1\n",
      "26.9 < 26.9 0\n",
      "26.9 < 26.9 0\n",
      "26.9 < 26.9 0\n",
      "26.8 < 26.9 0\n",
      "26.8 < 26.8 0\n",
      "26.8 < 26.8 0\n",
      "26.7 < 26.8 0\n",
      "26.7 < 26.7 0\n",
      "26.7 < 26.7 0\n",
      "26.7 < 26.7 0\n",
      "26.7 < 26.7 0\n",
      "26.7 < 26.7 0\n",
      "26.7 < 26.7 0\n",
      "26.8 > 26.7 1\n",
      "26.8 < 26.8 0\n",
      "26.9 > 26.8 1\n",
      "26.9 < 26.9 0\n",
      "26.9 < 26.9 0\n",
      "26.8 < 26.9 0\n",
      "26.8 < 26.8 0\n",
      "26.8 < 26.8 0\n",
      "26.8 < 26.8 0\n",
      "26.9 > 26.8 1\n",
      "27.0 > 26.9 1\n",
      "27.2 > 27.0 1\n",
      "27.4 > 27.2 1\n",
      "27.3 < 27.4 0\n",
      "27.4 > 27.3 1\n",
      "27.5 > 27.4 1\n",
      "27.5 < 27.5 0\n",
      "27.6 > 27.5 1\n",
      "27.6 < 27.6 0\n",
      "27.7 > 27.6 1\n",
      "27.8 > 27.7 1\n",
      "27.9 > 27.8 1\n",
      "28.0 > 27.9 1\n",
      "28.1 > 28.0 1\n",
      "28.3 > 28.1 1\n",
      "28.3 < 28.3 0\n",
      "28.3 < 28.3 0\n",
      "28.3 < 28.3 0\n",
      "28.4 > 28.3 1\n",
      "28.4 < 28.4 0\n",
      "28.6 > 28.4 1\n",
      "28.6 < 28.6 0\n",
      "28.8 > 28.6 1\n",
      "28.9 > 28.8 1\n",
      "28.9 < 28.9 0\n",
      "28.9 < 28.9 0\n",
      "29.0 > 28.9 1\n",
      "28.9 < 29.0 0\n",
      "28.9 < 28.9 0\n",
      "28.9 < 28.9 0\n",
      "29.0 > 28.9 1\n",
      "28.9 < 29.0 0\n",
      "29.0 > 28.9 1\n",
      "28.9 < 29.0 0\n",
      "28.9 < 28.9 0\n",
      "29.0 > 28.9 1\n",
      "29.0 < 29.0 0\n",
      "29.1 > 29.0 1\n",
      "29.2 > 29.1 1\n",
      "29.2 < 29.2 0\n",
      "29.3 > 29.2 1\n",
      "29.4 > 29.3 1\n",
      "29.4 < 29.4 0\n",
      "29.4 < 29.4 0\n",
      "29.3 < 29.4 0\n",
      "29.4 > 29.3 1\n",
      "29.4 < 29.4 0\n",
      "29.5 > 29.4 1\n",
      "29.5 < 29.5 0\n",
      "29.6 > 29.5 1\n",
      "29.6 < 29.6 0\n",
      "29.6 < 29.6 0\n",
      "29.6 < 29.6 0\n",
      "29.8 > 29.6 1\n",
      "29.8 < 29.8 0\n",
      "29.8 < 29.8 0\n",
      "29.8 < 29.8 0\n",
      "29.8 < 29.8 0\n",
      "29.8 < 29.8 0\n",
      "29.8 < 29.8 0\n",
      "29.8 < 29.8 0\n",
      "29.8 < 29.8 0\n",
      "30.0 > 29.8 1\n",
      "29.9 < 30.0 0\n",
      "30.0 > 29.9 1\n",
      "30.0 < 30.0 0\n",
      "30.0 < 30.0 0\n",
      "30.0 < 30.0 0\n",
      "30.0 < 30.0 0\n",
      "30.1 > 30.0 1\n",
      "30.1 < 30.1 0\n",
      "30.2 > 30.1 1\n",
      "30.2 < 30.2 0\n",
      "30.2 < 30.2 0\n",
      "30.3 > 30.2 1\n",
      "30.3 < 30.3 0\n",
      "30.4 > 30.3 1\n",
      "30.4 < 30.4 0\n",
      "30.4 < 30.4 0\n",
      "30.4 < 30.4 0\n",
      "30.4 < 30.4 0\n",
      "30.4 < 30.4 0\n",
      "30.5 > 30.4 1\n",
      "30.5 < 30.5 0\n",
      "30.5 < 30.5 0\n",
      "30.6 > 30.5 1\n",
      "30.7 > 30.6 1\n",
      "30.7 < 30.7 0\n",
      "30.7 < 30.7 0\n",
      "30.8 > 30.7 1\n",
      "30.8 < 30.8 0\n",
      "30.9 > 30.8 1\n",
      "30.9 < 30.9 0\n",
      "30.9 < 30.9 0\n",
      "30.9 < 30.9 0\n",
      "30.9 < 30.9 0\n",
      "30.9 < 30.9 0\n",
      "31.0 > 30.9 1\n",
      "31.1 > 31.0 1\n",
      "31.0 < 31.1 0\n",
      "31.1 > 31.0 1\n",
      "31.1 < 31.1 0\n",
      "31.2 > 31.1 1\n",
      "31.2 < 31.2 0\n",
      "31.2 < 31.2 0\n",
      "31.2 < 31.2 0\n",
      "31.3 > 31.2 1\n",
      "31.4 > 31.3 1\n",
      "31.4 < 31.4 0\n",
      "31.6 > 31.4 1\n",
      "31.6 < 31.6 0\n",
      "31.6 < 31.6 0\n",
      "31.6 < 31.6 0\n",
      "31.7 > 31.6 1\n",
      "31.7 < 31.7 0\n",
      "31.8 > 31.7 1\n",
      "31.8 < 31.8 0\n",
      "32.0 > 31.8 1\n",
      "32.1 > 32.0 1\n",
      "32.3 > 32.1 1\n",
      "32.3 < 32.3 0\n",
      "32.4 > 32.3 1\n",
      "32.5 > 32.4 1\n",
      "32.7 > 32.5 1\n",
      "32.7 < 32.7 0\n",
      "32.9 > 32.7 1\n",
      "32.9 < 32.9 0\n",
      "32.9 < 32.9 0\n",
      "32.9 < 32.9 0\n",
      "32.9 < 32.9 0\n",
      "33.0 > 32.9 1\n",
      "33.1 > 33.0 1\n",
      "33.2 > 33.1 1\n",
      "33.3 > 33.2 1\n",
      "33.4 > 33.3 1\n",
      "33.5 > 33.4 1\n",
      "33.6 > 33.5 1\n",
      "33.7 > 33.6 1\n",
      "33.8 > 33.7 1\n",
      "33.9 > 33.8 1\n",
      "34.1 > 33.9 1\n",
      "34.2 > 34.1 1\n",
      "34.3 > 34.2 1\n",
      "34.4 > 34.3 1\n",
      "34.5 > 34.4 1\n",
      "34.7 > 34.5 1\n",
      "34.9 > 34.7 1\n",
      "35.0 > 34.9 1\n",
      "35.1 > 35.0 1\n",
      "35.3 > 35.1 1\n",
      "35.4 > 35.3 1\n",
      "35.5 > 35.4 1\n",
      "35.6 > 35.5 1\n",
      "35.8 > 35.6 1\n",
      "36.1 > 35.8 1\n",
      "36.3 > 36.1 1\n",
      "36.4 > 36.3 1\n",
      "36.6 > 36.4 1\n",
      "36.8 > 36.6 1\n",
      "37.0 > 36.8 1\n",
      "37.1 > 37.0 1\n",
      "37.3 > 37.1 1\n",
      "37.5 > 37.3 1\n",
      "37.7 > 37.5 1\n",
      "37.8 > 37.7 1\n",
      "38.0 > 37.8 1\n",
      "38.2 > 38.0 1\n",
      "38.5 > 38.2 1\n",
      "38.6 > 38.5 1\n",
      "38.8 > 38.6 1\n",
      "39.0 > 38.8 1\n",
      "39.0 < 39.0 0\n",
      "39.2 > 39.0 1\n",
      "39.4 > 39.2 1\n",
      "39.6 > 39.4 1\n",
      "39.8 > 39.6 1\n",
      "39.8 < 39.8 0\n",
      "39.9 > 39.8 1\n",
      "40.0 > 39.9 1\n",
      "40.1 > 40.0 1\n",
      "40.3 > 40.1 1\n",
      "40.6 > 40.3 1\n",
      "40.7 > 40.6 1\n",
      "40.8 > 40.7 1\n",
      "40.8 < 40.8 0\n",
      "40.9 > 40.8 1\n",
      "40.9 < 40.9 0\n",
      "41.1 > 40.9 1\n",
      "41.1 < 41.1 0\n",
      "41.3 > 41.1 1\n",
      "41.4 > 41.3 1\n",
      "41.5 > 41.4 1\n",
      "41.6 > 41.5 1\n",
      "41.7 > 41.6 1\n",
      "41.9 > 41.7 1\n",
      "42.0 > 41.9 1\n",
      "42.1 > 42.0 1\n",
      "42.3 > 42.1 1\n",
      "42.4 > 42.3 1\n",
      "42.5 > 42.4 1\n",
      "42.6 > 42.5 1\n",
      "42.9 > 42.6 1\n",
      "43.3 > 42.9 1\n",
      "43.6 > 43.3 1\n",
      "43.9 > 43.6 1\n",
      "44.2 > 43.9 1\n",
      "44.3 > 44.2 1\n",
      "45.1 > 44.3 1\n",
      "45.2 > 45.1 1\n",
      "45.6 > 45.2 1\n",
      "45.9 > 45.6 1\n",
      "46.2 > 45.9 1\n",
      "46.6 > 46.2 1\n",
      "47.2 > 46.6 1\n",
      "47.8 > 47.2 1\n",
      "48.0 > 47.8 1\n",
      "48.6 > 48.0 1\n",
      "49.0 > 48.6 1\n",
      "49.4 > 49.0 1\n",
      "50.0 > 49.4 1\n",
      "50.6 > 50.0 1\n",
      "51.1 > 50.6 1\n",
      "51.5 > 51.1 1\n",
      "51.9 > 51.5 1\n",
      "52.1 > 51.9 1\n",
      "52.5 > 52.1 1\n",
      "52.7 > 52.5 1\n",
      "52.9 > 52.7 1\n",
      "53.2 > 52.9 1\n",
      "53.6 > 53.2 1\n",
      "54.2 > 53.6 1\n",
      "54.3 > 54.2 1\n",
      "54.6 > 54.3 1\n",
      "54.9 > 54.6 1\n",
      "55.3 > 54.9 1\n",
      "55.5 > 55.3 1\n",
      "55.6 > 55.5 1\n",
      "55.8 > 55.6 1\n",
      "55.9 > 55.8 1\n",
      "56.1 > 55.9 1\n",
      "56.5 > 56.1 1\n",
      "56.8 > 56.5 1\n",
      "57.1 > 56.8 1\n",
      "57.4 > 57.1 1\n",
      "57.6 > 57.4 1\n",
      "57.9 > 57.6 1\n",
      "58.0 > 57.9 1\n",
      "58.2 > 58.0 1\n",
      "58.5 > 58.2 1\n",
      "59.1 > 58.5 1\n",
      "59.5 > 59.1 1\n",
      "60.0 > 59.5 1\n",
      "60.3 > 60.0 1\n",
      "60.7 > 60.3 1\n",
      "61.0 > 60.7 1\n",
      "61.2 > 61.0 1\n",
      "61.4 > 61.2 1\n",
      "61.6 > 61.4 1\n",
      "61.9 > 61.6 1\n",
      "62.1 > 61.9 1\n",
      "62.5 > 62.1 1\n",
      "62.9 > 62.5 1\n",
      "63.4 > 62.9 1\n",
      "63.9 > 63.4 1\n",
      "64.5 > 63.9 1\n",
      "65.2 > 64.5 1\n",
      "65.7 > 65.2 1\n",
      "66.0 > 65.7 1\n",
      "66.5 > 66.0 1\n",
      "67.1 > 66.5 1\n",
      "67.4 > 67.1 1\n",
      "67.7 > 67.4 1\n",
      "68.3 > 67.7 1\n",
      "69.1 > 68.3 1\n",
      "69.8 > 69.1 1\n",
      "70.6 > 69.8 1\n",
      "71.5 > 70.6 1\n",
      "72.3 > 71.5 1\n",
      "73.1 > 72.3 1\n",
      "73.8 > 73.1 1\n",
      "74.6 > 73.8 1\n",
      "75.2 > 74.6 1\n",
      "75.9 > 75.2 1\n",
      "76.7 > 75.9 1\n",
      "77.8 > 76.7 1\n",
      "78.9 > 77.8 1\n",
      "80.1 > 78.9 1\n",
      "81.0 > 80.1 1\n",
      "81.8 > 81.0 1\n",
      "82.7 > 81.8 1\n",
      "82.7 < 82.7 0\n",
      "83.3 > 82.7 1\n",
      "84.0 > 83.3 1\n",
      "84.8 > 84.0 1\n",
      "85.5 > 84.8 1\n",
      "86.3 > 85.5 1\n",
      "87.0 > 86.3 1\n",
      "87.9 > 87.0 1\n",
      "88.5 > 87.9 1\n",
      "89.1 > 88.5 1\n",
      "89.8 > 89.1 1\n",
      "90.6 > 89.8 1\n",
      "91.6 > 90.6 1\n",
      "92.3 > 91.6 1\n",
      "93.2 > 92.3 1\n",
      "93.4 > 93.2 1\n",
      "93.7 > 93.4 1\n",
      "94.0 > 93.7 1\n",
      "94.3 > 94.0 1\n",
      "94.6 > 94.3 1\n",
      "94.5 < 94.6 0\n",
      "94.9 > 94.5 1\n",
      "95.8 > 94.9 1\n",
      "97.0 > 95.8 1\n",
      "97.5 > 97.0 1\n",
      "97.7 > 97.5 1\n",
      "97.9 > 97.7 1\n",
      "98.2 > 97.9 1\n",
      "98.0 < 98.2 0\n",
      "97.6 < 98.0 0\n",
      "97.8 > 97.6 1\n",
      "97.9 > 97.8 1\n",
      "97.9 < 97.9 0\n",
      "98.6 > 97.9 1\n",
      "99.2 > 98.6 1\n",
      "99.5 > 99.2 1\n",
      "99.9 > 99.5 1\n",
      "100.2 > 99.9 1\n",
      "100.7 > 100.2 1\n",
      "101.0 > 100.7 1\n",
      "101.2 > 101.0 1\n",
      "101.3 > 101.2 1\n",
      "101.9 > 101.3 1\n",
      "102.4 > 101.9 1\n",
      "102.6 > 102.4 1\n",
      "103.1 > 102.6 1\n",
      "103.4 > 103.1 1\n",
      "103.7 > 103.4 1\n",
      "104.1 > 103.7 1\n",
      "104.5 > 104.1 1\n",
      "105.0 > 104.5 1\n",
      "105.3 > 105.0 1\n",
      "105.3 < 105.3 0\n",
      "105.3 < 105.3 0\n",
      "105.5 > 105.3 1\n",
      "106.0 > 105.5 1\n",
      "106.4 > 106.0 1\n",
      "106.9 > 106.4 1\n",
      "107.3 > 106.9 1\n",
      "107.6 > 107.3 1\n",
      "107.8 > 107.6 1\n",
      "108.0 > 107.8 1\n",
      "108.3 > 108.0 1\n",
      "108.7 > 108.3 1\n",
      "109.0 > 108.7 1\n",
      "109.3 > 109.0 1\n",
      "109.6 > 109.3 1\n",
      "109.3 < 109.6 0\n",
      "108.8 < 109.3 0\n",
      "108.6 < 108.8 0\n",
      "108.9 > 108.6 1\n",
      "109.5 > 108.9 1\n",
      "109.5 < 109.5 0\n",
      "109.7 > 109.5 1\n",
      "110.2 > 109.7 1\n",
      "110.3 > 110.2 1\n",
      "110.4 > 110.3 1\n",
      "110.5 > 110.4 1\n",
      "111.2 > 110.5 1\n",
      "111.6 > 111.2 1\n",
      "112.1 > 111.6 1\n",
      "112.7 > 112.1 1\n",
      "113.1 > 112.7 1\n",
      "113.5 > 113.1 1\n",
      "113.8 > 113.5 1\n",
      "114.4 > 113.8 1\n",
      "115.0 > 114.4 1\n",
      "115.3 > 115.0 1\n",
      "115.4 > 115.3 1\n",
      "115.4 < 115.4 0\n",
      "115.7 > 115.4 1\n",
      "116.0 > 115.7 1\n",
      "116.5 > 116.0 1\n",
      "117.1 > 116.5 1\n",
      "117.5 > 117.1 1\n",
      "118.0 > 117.5 1\n",
      "118.5 > 118.0 1\n",
      "119.0 > 118.5 1\n",
      "119.8 > 119.0 1\n",
      "120.2 > 119.8 1\n",
      "120.3 > 120.2 1\n",
      "120.5 > 120.3 1\n",
      "121.1 > 120.5 1\n",
      "121.6 > 121.1 1\n",
      "122.3 > 121.6 1\n",
      "123.1 > 122.3 1\n",
      "123.8 > 123.1 1\n",
      "124.1 > 123.8 1\n",
      "124.4 > 124.1 1\n",
      "124.6 > 124.4 1\n",
      "125.0 > 124.6 1\n",
      "125.6 > 125.0 1\n",
      "125.9 > 125.6 1\n",
      "126.1 > 125.9 1\n",
      "127.4 > 126.1 1\n",
      "128.0 > 127.4 1\n",
      "128.7 > 128.0 1\n",
      "128.9 > 128.7 1\n",
      "129.2 > 128.9 1\n",
      "129.9 > 129.2 1\n",
      "130.4 > 129.9 1\n",
      "131.6 > 130.4 1\n",
      "132.7 > 131.6 1\n",
      "133.5 > 132.7 1\n",
      "133.8 > 133.5 1\n",
      "133.8 < 133.8 0\n",
      "134.6 > 133.8 1\n",
      "134.8 > 134.6 1\n",
      "135.0 > 134.8 1\n",
      "135.2 > 135.0 1\n",
      "135.6 > 135.2 1\n",
      "136.0 > 135.6 1\n",
      "136.2 > 136.0 1\n",
      "136.6 > 136.2 1\n",
      "137.2 > 136.6 1\n",
      "137.4 > 137.2 1\n",
      "137.8 > 137.4 1\n",
      "137.9 > 137.8 1\n",
      "138.1 > 137.9 1\n",
      "138.6 > 138.1 1\n",
      "139.3 > 138.6 1\n",
      "139.5 > 139.3 1\n",
      "139.7 > 139.5 1\n",
      "140.2 > 139.7 1\n",
      "140.5 > 140.2 1\n",
      "140.9 > 140.5 1\n",
      "141.3 > 140.9 1\n",
      "141.8 > 141.3 1\n",
      "142.0 > 141.8 1\n",
      "141.9 < 142.0 0\n",
      "142.6 > 141.9 1\n",
      "143.1 > 142.6 1\n",
      "143.6 > 143.1 1\n",
      "144.0 > 143.6 1\n",
      "144.2 > 144.0 1\n",
      "144.4 > 144.2 1\n",
      "144.4 < 144.4 0\n",
      "144.8 > 144.4 1\n",
      "145.1 > 144.8 1\n",
      "145.7 > 145.1 1\n",
      "145.8 > 145.7 1\n",
      "145.8 < 145.8 0\n",
      "146.2 > 145.8 1\n",
      "146.7 > 146.2 1\n",
      "147.2 > 146.7 1\n",
      "147.4 > 147.2 1\n",
      "147.5 > 147.4 1\n",
      "148.0 > 147.5 1\n",
      "148.4 > 148.0 1\n",
      "149.0 > 148.4 1\n",
      "149.4 > 149.0 1\n",
      "149.5 > 149.4 1\n",
      "149.7 > 149.5 1\n",
      "149.7 < 149.7 0\n",
      "150.3 > 149.7 1\n",
      "150.9 > 150.3 1\n",
      "151.4 > 150.9 1\n",
      "151.9 > 151.4 1\n",
      "152.2 > 151.9 1\n",
      "152.5 > 152.2 1\n",
      "152.5 < 152.5 0\n",
      "152.9 > 152.5 1\n",
      "153.2 > 152.9 1\n",
      "153.7 > 153.2 1\n",
      "153.6 < 153.7 0\n",
      "153.5 < 153.6 0\n",
      "154.4 > 153.5 1\n",
      "154.9 > 154.4 1\n",
      "155.7 > 154.9 1\n",
      "156.3 > 155.7 1\n",
      "156.6 > 156.3 1\n",
      "156.7 > 156.6 1\n",
      "157.0 > 156.7 1\n",
      "157.3 > 157.0 1\n",
      "157.8 > 157.3 1\n",
      "158.3 > 157.8 1\n",
      "158.6 > 158.3 1\n",
      "158.6 < 158.6 0\n",
      "159.1 > 158.6 1\n",
      "159.6 > 159.1 1\n",
      "160.0 > 159.6 1\n",
      "160.2 > 160.0 1\n",
      "160.1 < 160.2 0\n",
      "160.3 > 160.1 1\n",
      "160.5 > 160.3 1\n",
      "160.8 > 160.5 1\n",
      "161.2 > 160.8 1\n",
      "161.6 > 161.2 1\n",
      "161.5 < 161.6 0\n",
      "161.3 < 161.5 0\n",
      "161.6 > 161.3 1\n",
      "161.9 > 161.6 1\n",
      "162.2 > 161.9 1\n",
      "162.5 > 162.2 1\n",
      "162.8 > 162.5 1\n",
      "163.0 > 162.8 1\n",
      "163.2 > 163.0 1\n",
      "163.4 > 163.2 1\n",
      "163.6 > 163.4 1\n",
      "164.0 > 163.6 1\n",
      "164.0 < 164.0 0\n",
      "163.9 < 164.0 0\n",
      "164.3 > 163.9 1\n",
      "164.5 > 164.3 1\n",
      "165.0 > 164.5 1\n",
      "166.2 > 165.0 1\n",
      "166.2 < 166.2 0\n",
      "166.2 < 166.2 0\n",
      "166.7 > 166.2 1\n",
      "167.1 > 166.7 1\n",
      "167.9 > 167.1 1\n",
      "168.2 > 167.9 1\n",
      "168.3 > 168.2 1\n",
      "168.3 < 168.3 0\n",
      "168.8 > 168.3 1\n",
      "169.8 > 168.8 1\n",
      "171.2 > 169.8 1\n",
      "171.3 > 171.2 1\n",
      "171.5 > 171.3 1\n",
      "172.4 > 171.5 1\n",
      "172.8 > 172.4 1\n",
      "172.8 < 172.8 0\n",
      "173.7 > 172.8 1\n",
      "174.0 > 173.7 1\n",
      "174.1 > 174.0 1\n",
      "174.0 < 174.1 0\n",
      "175.1 > 174.0 1\n",
      "175.8 > 175.1 1\n",
      "176.2 > 175.8 1\n",
      "176.9 > 176.2 1\n",
      "177.7 > 176.9 1\n",
      "178.0 > 177.7 1\n",
      "177.5 < 178.0 0\n",
      "177.5 < 177.5 0\n",
      "178.3 > 177.5 1\n",
      "177.7 < 178.3 0\n",
      "177.4 < 177.7 0\n",
      "176.7 < 177.4 0\n",
      "177.1 > 176.7 1\n",
      "177.8 > 177.1 1\n",
      "178.8 > 177.8 1\n",
      "179.8 > 178.8 1\n",
      "179.8 < 179.8 0\n",
      "179.9 > 179.8 1\n",
      "180.1 > 179.9 1\n",
      "180.7 > 180.1 1\n",
      "181.0 > 180.7 1\n",
      "181.3 > 181.0 1\n",
      "181.3 < 181.3 0\n",
      "180.9 < 181.3 0\n",
      "181.7 > 180.9 1\n",
      "183.1 > 181.7 1\n",
      "184.2 > 183.1 1\n",
      "183.8 < 184.2 0\n",
      "183.5 < 183.8 0\n",
      "183.7 > 183.5 1\n",
      "183.9 > 183.7 1\n",
      "184.6 > 183.9 1\n",
      "185.2 > 184.6 1\n",
      "185.0 < 185.2 0\n",
      "184.5 < 185.0 0\n",
      "184.3 < 184.5 0\n",
      "185.2 > 184.3 1\n",
      "186.2 > 185.2 1\n",
      "187.4 > 186.2 1\n",
      "188.0 > 187.4 1\n",
      "189.1 > 188.0 1\n",
      "189.7 > 189.1 1\n",
      "189.4 < 189.7 0\n",
      "189.5 > 189.4 1\n",
      "189.9 > 189.5 1\n",
      "190.9 > 189.9 1\n",
      "191.0 > 190.9 1\n",
      "190.3 < 191.0 0\n",
      "190.7 > 190.3 1\n",
      "191.8 > 190.7 1\n",
      "193.3 > 191.8 1\n",
      "194.6 > 193.3 1\n",
      "194.4 < 194.6 0\n",
      "194.5 > 194.4 1\n",
      "195.4 > 194.5 1\n",
      "196.4 > 195.4 1\n",
      "198.8 > 196.4 1\n",
      "199.2 > 198.8 1\n",
      "197.6 < 199.2 0\n",
      "196.8 < 197.6 0\n",
      "198.3 > 196.8 1\n",
      "198.7 > 198.3 1\n",
      "199.8 > 198.7 1\n",
      "201.5 > 199.8 1\n",
      "202.5 > 201.5 1\n",
      "202.9 > 202.5 1\n",
      "203.5 > 202.9 1\n",
      "203.9 > 203.5 1\n",
      "202.9 < 203.9 0\n",
      "201.8 < 202.9 0\n",
      "201.5 < 201.8 0\n",
      "201.8 > 201.5 1\n",
      "202.42 > 201.8 1\n",
      "203.5 > 202.42 1\n",
      "205.35 > 203.5 1\n",
      "206.69 > 205.35 1\n",
      "207.95 > 206.69 1\n",
      "208.35 > 207.95 1\n",
      "208.3 < 208.35 0\n",
      "207.92 < 208.3 0\n",
      "208.49 > 207.92 1\n",
      "208.94 > 208.49 1\n",
      "210.18 > 208.94 1\n",
      "210.04 < 210.18 0\n",
      "211.08 > 210.04 1\n",
      "211.69 > 211.08 1\n",
      "213.53 > 211.69 1\n",
      "214.82 > 213.53 1\n",
      "216.63 > 214.82 1\n",
      "218.81 > 216.63 1\n",
      "219.96 > 218.81 1\n",
      "219.09 < 219.96 0\n",
      "218.78 < 219.09 0\n",
      "216.57 < 218.78 0\n",
      "212.43 < 216.57 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.23 < 212.43 0\n",
      "211.14 > 210.23 1\n",
      "212.19 > 211.14 1\n",
      "212.71 > 212.19 1\n",
      "213.24 > 212.71 1\n",
      "213.86 > 213.24 1\n",
      "215.69 > 213.86 1\n",
      "215.35 < 215.69 0\n",
      "215.83 > 215.35 1\n",
      "215.97 > 215.83 1\n",
      "216.18 > 215.97 1\n",
      "216.33 > 216.18 1\n",
      "215.95 < 216.33 0\n",
      "216.69 > 215.95 1\n",
      "216.74 > 216.69 1\n",
      "217.63 > 216.74 1\n",
      "218.01 > 217.63 1\n",
      "218.18 > 218.01 1\n",
      "217.97 < 218.18 0\n",
      "218.01 > 217.97 1\n",
      "218.31 > 218.01 1\n",
      "218.44 > 218.31 1\n",
      "218.71 > 218.44 1\n",
      "218.8 > 218.71 1\n",
      "219.18 > 218.8 1\n",
      "220.22 > 219.18 1\n",
      "221.31 > 220.22 1\n",
      "223.47 > 221.31 1\n",
      "224.91 > 223.47 1\n",
      "225.96 > 224.91 1\n",
      "225.72 < 225.96 0\n",
      "225.92 > 225.72 1\n",
      "226.54 > 225.92 1\n",
      "226.89 > 226.54 1\n",
      "226.42 < 226.89 0\n",
      "226.23 < 226.42 0\n",
      "225.67 < 226.23 0\n",
      "226.66 > 225.67 1\n",
      "227.66 > 226.66 1\n",
      "229.39 > 227.66 1\n",
      "230.09 > 229.39 1\n",
      "229.81 < 230.09 0\n",
      "229.48 < 229.81 0\n",
      "229.1 < 229.48 0\n",
      "230.38 > 229.1 1\n",
      "231.41 > 230.38 1\n",
      "231.32 < 231.41 0\n",
      "230.22 < 231.32 0\n",
      "229.6 < 230.22 0\n",
      "230.28 > 229.6 1\n",
      "232.17 > 230.28 1\n",
      "232.77 > 232.17 1\n",
      "232.53 < 232.77 0\n",
      "232.94 > 232.53 1\n",
      "233.5 > 232.94 1\n",
      "233.6 > 233.5 1\n",
      "233.88 > 233.6 1\n",
      "234.15 > 233.88 1\n",
      "233.55 < 234.15 0\n",
      "233.07 < 233.55 0\n",
      "233.05 < 233.07 0\n",
      "233.92 > 233.05 1\n",
      "234.78 > 233.92 1\n",
      "236.29 > 234.78 1\n",
      "237.07 > 236.29 1\n",
      "237.9 > 237.07 1\n",
      "238.34 > 237.9 1\n",
      "238.25 < 238.34 0\n",
      "237.85 < 238.25 0\n",
      "238.03 > 237.85 1\n",
      "237.43 < 238.03 0\n",
      "236.15 < 237.43 0\n",
      "234.81 < 236.15 0\n",
      "233.71 < 234.81 0\n",
      "234.72 > 233.71 1\n",
      "236.12 > 234.72 1\n",
      "236.6 > 236.12 1\n",
      "237.81 > 236.6 1\n",
      "238.64 > 237.81 1\n",
      "238.65 > 238.64 1\n",
      "238.32 < 238.65 0\n",
      "237.94 < 238.32 0\n",
      "237.84 < 237.94 0\n",
      "237.34 < 237.84 0\n",
      "236.53 < 237.34 0\n",
      "236.92 > 236.53 1\n",
      "237.11 > 236.92 1\n",
      "238.13 > 237.11 1\n",
      "239.26 > 238.13 1\n",
      "240.23 > 239.26 1\n",
      "241.02 > 240.23 1\n",
      "240.63 < 241.02 0\n",
      "240.85 > 240.63 1\n",
      "241.43 > 240.85 1\n",
      "241.73 > 241.43 1\n",
      "241.35 < 241.73 0\n",
      "241.43 > 241.35 1\n",
      "242.84 > 241.43 1\n",
      "243.6 > 242.84 1\n",
      "243.8 > 243.6 1\n",
      "244.52 > 243.8 1\n",
      "244.73 > 244.52 1\n",
      "244.96 > 244.73 1\n",
      "244.79 < 244.96 0\n",
      "245.52 > 244.79 1\n",
      "246.82 > 245.52 1\n",
      "246.66 < 246.82 0\n",
      "246.67 > 246.66 1\n",
      "246.52 < 246.67 0\n",
      "247.87 > 246.52 1\n",
      "248.99 > 247.87 1\n",
      "249.55 > 248.99 1\n",
      "249.84 > 249.55 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1768",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-04ec3e7aa352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Consumer Price Index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Consumer Price Index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Consumer Price Index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'>'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   3116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 3118\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   3119\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3120\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1768"
     ]
    }
   ],
   "source": [
    "## Use the below code to create a new variable so as to detect increament or decrement of CPI \n",
    "\n",
    "cnt=1\n",
    "target=[]\n",
    "for i in sp['Consumer Price Index']:\n",
    "    if sp['Consumer Price Index'][cnt] > i:\n",
    "        print(sp['Consumer Price Index'][cnt], '>' , i , '1')\n",
    "        target.append(1)\n",
    "    else:\n",
    "        print(sp['Consumer Price Index'][cnt], '<' , i , '0')\n",
    "        target.append(0)\n",
    "    cnt+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=pd.DataFrame(data=target, columns=['CPI_Inc'],dtype=int)\n",
    "sp=sp[:1767]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Earnings</th>\n",
       "      <th>Consumer Price Index</th>\n",
       "      <th>Long Interest Rate</th>\n",
       "      <th>Real Price</th>\n",
       "      <th>Real Dividend</th>\n",
       "      <th>Real Earnings</th>\n",
       "      <th>CPI_Inc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2593.61</td>\n",
       "      <td>48.68</td>\n",
       "      <td>108.95</td>\n",
       "      <td>246.67</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2626.9</td>\n",
       "      <td>49.3</td>\n",
       "      <td>110.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2664.34</td>\n",
       "      <td>48.93</td>\n",
       "      <td>109.88</td>\n",
       "      <td>246.52</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2700.13</td>\n",
       "      <td>49.59</td>\n",
       "      <td>111.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2789.8</td>\n",
       "      <td>49.29</td>\n",
       "      <td>49.29</td>\n",
       "      <td>247.87</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2811.96</td>\n",
       "      <td>49.68</td>\n",
       "      <td>49.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2705.16</td>\n",
       "      <td>49.64</td>\n",
       "      <td>49.64</td>\n",
       "      <td>248.99</td>\n",
       "      <td>2.86</td>\n",
       "      <td>2714.34</td>\n",
       "      <td>49.81</td>\n",
       "      <td>49.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2702.77</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>249.55</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2705.82</td>\n",
       "      <td>50.06</td>\n",
       "      <td>50.06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    SP500 Dividend Earnings Consumer Price Index  \\\n",
       "1762  2017-11-01  2593.61    48.68   108.95               246.67   \n",
       "1763  2017-12-01  2664.34    48.93   109.88               246.52   \n",
       "1764  2018-01-01   2789.8    49.29    49.29               247.87   \n",
       "1765  2018-02-01  2705.16    49.64    49.64               248.99   \n",
       "1766  2018-03-01  2702.77       50       50               249.55   \n",
       "\n",
       "     Long Interest Rate Real Price Real Dividend Real Earnings  CPI_Inc  \n",
       "1762               2.35     2626.9          49.3        110.35        0  \n",
       "1763                2.4    2700.13         49.59        111.36        1  \n",
       "1764               2.58    2811.96         49.68         49.68        1  \n",
       "1765               2.86    2714.34         49.81         49.81        1  \n",
       "1766               2.84    2705.82         50.06         50.06        1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames=[sp,target]\n",
    "sp=pd.concat(frames, axis=1, join='outer')\n",
    "sp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp3=sp.drop('Date',axis=1)\n",
    "\n",
    "test=sp3[-40:]\n",
    "train=sp3[:-40]\n",
    "#Train\n",
    "sp1=np.array(train)\n",
    "#Test\n",
    "sp2=np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To change the dimension of the data for deep learning model\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1720, 8, 8) (1720,)\n",
      "(33, 8, 8) (33,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose a number of time steps\n",
    "n_steps = 8\n",
    "\n",
    "# convert into input/output\n",
    "X1, y1 = split_sequences(sp1, n_steps)\n",
    "# convert into input/output\n",
    "X2, y2 = split_sequences(sp2, n_steps)\n",
    "\n",
    "print(X1.shape, y1.shape)\n",
    "print(X2.shape, y2.shape)\n",
    "\n",
    "\n",
    "n_features = X2.shape[2]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. CNN Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A - Deep Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Hyperparameters:___<br>\n",
    "filters=128<br>\n",
    "kernel_size=3<br>\n",
    "Activation functions=  relu<br>\n",
    "loss=mean_squared_error<br>\n",
    "optimizer= adam<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# univariate cnn example\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import metrics\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model1.add(MaxPooling1D(pool_size=2))\n",
    "model1.add(BatchNormalization())\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(20, activation='relu'))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 0s - loss: 0.9079 - mean_absolute_error: 0.7118 - accuracy: 0.4372\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.3815 - mean_absolute_error: 0.5101 - accuracy: 0.5384\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.2962 - mean_absolute_error: 0.4621 - accuracy: 0.5878\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.2671 - mean_absolute_error: 0.4447 - accuracy: 0.6122\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.2616 - mean_absolute_error: 0.4450 - accuracy: 0.6145\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.2493 - mean_absolute_error: 0.4385 - accuracy: 0.6238\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.2369 - mean_absolute_error: 0.4315 - accuracy: 0.6401\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.2305 - mean_absolute_error: 0.4274 - accuracy: 0.6372\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.2271 - mean_absolute_error: 0.4273 - accuracy: 0.6483\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.2276 - mean_absolute_error: 0.4295 - accuracy: 0.6488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27fc8fe3ef0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(optimizer='adam', loss='mean_squared_error',metrics=[metrics.mae, 'accuracy'])\n",
    "history=model1.fit(X1, y1, batch_size=32, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Valid'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data :- \n",
      "\t loss: 0.6278259826428962   mean_absolute_error: 0.5689218640327454   accuracy: 0.6363636255264282\n"
     ]
    }
   ],
   "source": [
    "loss, mean_absolute_error, accuracy =model1.evaluate(X2, y2, batch_size=32, verbose=2)\n",
    "print('Accuracy on Test Data :- ')\n",
    "print('\\t loss:',loss , '  mean_absolute_error:',mean_absolute_error, '  accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For the baseline model, Loss is Higher and Accuracy on validation data is 63%__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Hyperparameters:___<br>\n",
    "filters=128<br>\n",
    "kernel_size=3<br>\n",
    "Activation functions=  elu, softmax<br>\n",
    "loss=mean_squared_error<br>\n",
    "optimizer= adam<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv1D(filters=128, kernel_size=3, activation='elu', input_shape=(n_steps, n_features)))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(BatchNormalization())\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Dense(20, activation='softmax'))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 0s - loss: 0.3393 - mean_absolute_error: 0.5012 - accuracy: 0.5041\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.2630 - mean_absolute_error: 0.4662 - accuracy: 0.6250\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.2531 - mean_absolute_error: 0.4617 - accuracy: 0.6285\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.2433 - mean_absolute_error: 0.4480 - accuracy: 0.6355\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.2417 - mean_absolute_error: 0.4452 - accuracy: 0.6273\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.2390 - mean_absolute_error: 0.4483 - accuracy: 0.6134\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.2300 - mean_absolute_error: 0.4389 - accuracy: 0.6297\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.2230 - mean_absolute_error: 0.4265 - accuracy: 0.6424\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.2205 - mean_absolute_error: 0.4279 - accuracy: 0.6517\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.2186 - mean_absolute_error: 0.4234 - accuracy: 0.6547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27fca1f9eb8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam=optimizers.Adam(learning_rate=0.4, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model2.compile(optimizer='adam', loss='mean_squared_error',metrics=[metrics.mae, 'accuracy'])\n",
    "model2.fit(X1, y1, batch_size=32, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data :- \n",
      "\t loss: 0.2764722759073431   mean_absolute_error: 0.523048996925354   accuracy: 0.3030303120613098\n"
     ]
    }
   ],
   "source": [
    "loss, mean_absolute_error, accuracy =model2.evaluate(X2, y2, batch_size=32, verbose=2)\n",
    "print('Accuracy on Test Data :- ')\n",
    "print('\\t loss:',loss , '  mean_absolute_error:',mean_absolute_error, '  accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__After changing the activation functions in the current network, loss get reduced but with decrease in accuracy on validation set which is 30%.__  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function / loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Hyperparameters:___<br>\n",
    "filters=128<br>\n",
    "kernel_size=3<br>\n",
    "Activation functions=  elu, softmax<br>\n",
    "loss= hinge, logcosh<br>\n",
    "optimizer= adam<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Cost function= hinge:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 0s - loss: 0.8402 - mean_absolute_error: 0.3850 - accuracy: 0.6599\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.7873 - mean_absolute_error: 0.4537 - accuracy: 0.6541\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.7639 - mean_absolute_error: 0.5488 - accuracy: 0.5471\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.7270 - mean_absolute_error: 0.6014 - accuracy: 0.4076\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.7255 - mean_absolute_error: 0.6384 - accuracy: 0.4070\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6938 - mean_absolute_error: 0.7034 - accuracy: 0.3709\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.7004 - mean_absolute_error: 0.7443 - accuracy: 0.3744\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6855 - mean_absolute_error: 0.7608 - accuracy: 0.3744\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6869 - mean_absolute_error: 0.7982 - accuracy: 0.3628\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6707 - mean_absolute_error: 0.7844 - accuracy: 0.3709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27fcedfcfd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(optimizer='adam', loss='hinge',metrics=[metrics.mae, 'accuracy'])\n",
    "model2.fit(X1, y1, batch_size=32, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data :- \n",
      "\t loss: 0.6813373999162153   mean_absolute_error: 0.3783071041107178   accuracy: 0.6969696879386902\n"
     ]
    }
   ],
   "source": [
    "loss, mean_absolute_error, accuracy =model2.evaluate(X2, y2, batch_size=32, verbose=2)\n",
    "print('Accuracy on Test Data :- ')\n",
    "print('\\t loss:',loss , '  mean_absolute_error:',mean_absolute_error, '  accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Cost function= logcosh:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 0s - loss: 0.1927 - mean_absolute_error: 0.5205 - accuracy: 0.5320\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.1088 - mean_absolute_error: 0.4016 - accuracy: 0.6587\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1016 - mean_absolute_error: 0.3884 - accuracy: 0.6808\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.1054 - mean_absolute_error: 0.3997 - accuracy: 0.6669\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.1062 - mean_absolute_error: 0.4093 - accuracy: 0.6547\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.1010 - mean_absolute_error: 0.3979 - accuracy: 0.6785\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0997 - mean_absolute_error: 0.3970 - accuracy: 0.6860\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.1005 - mean_absolute_error: 0.4005 - accuracy: 0.6791\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.1009 - mean_absolute_error: 0.4014 - accuracy: 0.6779\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0984 - mean_absolute_error: 0.3983 - accuracy: 0.6837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27fc8cea400>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(optimizer='adam', loss='logcosh',metrics=[metrics.mae, 'accuracy'])\n",
    "model2.fit(X1, y1, batch_size=32, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data :- \n",
      "\t loss: 0.10009397159923207   mean_absolute_error: 0.4186834394931793   accuracy: 0.6969696879386902\n"
     ]
    }
   ],
   "source": [
    "loss, mean_absolute_error, accuracy =model2.evaluate(X2, y2, batch_size=32, verbose=2)\n",
    "print('Accuracy on Test Data :- ')\n",
    "print('\\t loss:',loss , '  mean_absolute_error:',mean_absolute_error, '  accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For the cost function, logcosh is better than hinge for better accuracy.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Hyperparameters:___<br>\n",
    "filters=128<br>\n",
    "kernel_size=3<br>\n",
    "Activation functions=  elu, softmax<br>\n",
    "loss= logcosh<br>\n",
    "optimizer= adam<br>\n",
    "epochs=200 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 0s - loss: 0.0961 - mean_absolute_error: 0.3912 - accuracy: 0.6971\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.0988 - mean_absolute_error: 0.3957 - accuracy: 0.6849\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.0971 - mean_absolute_error: 0.3942 - accuracy: 0.6907\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.0998 - mean_absolute_error: 0.4024 - accuracy: 0.6837\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.0964 - mean_absolute_error: 0.3902 - accuracy: 0.6988\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.0943 - mean_absolute_error: 0.3880 - accuracy: 0.6994\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.0953 - mean_absolute_error: 0.3900 - accuracy: 0.6977\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.0964 - mean_absolute_error: 0.3951 - accuracy: 0.6901\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.0965 - mean_absolute_error: 0.3946 - accuracy: 0.6878\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.0937 - mean_absolute_error: 0.3886 - accuracy: 0.6971\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.0939 - mean_absolute_error: 0.3900 - accuracy: 0.7017\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.0927 - mean_absolute_error: 0.3836 - accuracy: 0.7116\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.0952 - mean_absolute_error: 0.3885 - accuracy: 0.7000\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.0940 - mean_absolute_error: 0.3846 - accuracy: 0.7058\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.0952 - mean_absolute_error: 0.3937 - accuracy: 0.6936\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.0919 - mean_absolute_error: 0.3785 - accuracy: 0.7174\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0946 - mean_absolute_error: 0.3920 - accuracy: 0.6994\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0931 - mean_absolute_error: 0.3868 - accuracy: 0.7012\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0942 - mean_absolute_error: 0.3859 - accuracy: 0.6948\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0931 - mean_absolute_error: 0.3880 - accuracy: 0.7110\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0922 - mean_absolute_error: 0.3844 - accuracy: 0.7052\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0928 - mean_absolute_error: 0.3843 - accuracy: 0.7070\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0942 - mean_absolute_error: 0.3889 - accuracy: 0.6994\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0930 - mean_absolute_error: 0.3891 - accuracy: 0.7012\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0927 - mean_absolute_error: 0.3847 - accuracy: 0.7076\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0940 - mean_absolute_error: 0.3902 - accuracy: 0.6959\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0930 - mean_absolute_error: 0.3866 - accuracy: 0.7035\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0925 - mean_absolute_error: 0.3877 - accuracy: 0.7076\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0936 - mean_absolute_error: 0.3809 - accuracy: 0.7076\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0925 - mean_absolute_error: 0.3878 - accuracy: 0.7029\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.0918 - mean_absolute_error: 0.3836 - accuracy: 0.7105\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.0929 - mean_absolute_error: 0.3869 - accuracy: 0.6977\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.0929 - mean_absolute_error: 0.3873 - accuracy: 0.7081\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.0925 - mean_absolute_error: 0.3848 - accuracy: 0.7099\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0935 - mean_absolute_error: 0.3866 - accuracy: 0.7000\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0940 - mean_absolute_error: 0.3868 - accuracy: 0.7047\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0922 - mean_absolute_error: 0.3843 - accuracy: 0.7047\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0918 - mean_absolute_error: 0.3849 - accuracy: 0.7105\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0933 - mean_absolute_error: 0.3894 - accuracy: 0.6983\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0935 - mean_absolute_error: 0.3899 - accuracy: 0.7087\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0927 - mean_absolute_error: 0.3850 - accuracy: 0.7099\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0929 - mean_absolute_error: 0.3834 - accuracy: 0.7076\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.0908 - mean_absolute_error: 0.3801 - accuracy: 0.7192\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.0919 - mean_absolute_error: 0.3780 - accuracy: 0.7064\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.0937 - mean_absolute_error: 0.3917 - accuracy: 0.7023\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.0942 - mean_absolute_error: 0.3896 - accuracy: 0.7035\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.0933 - mean_absolute_error: 0.3886 - accuracy: 0.7070\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.0925 - mean_absolute_error: 0.3832 - accuracy: 0.7134\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.0936 - mean_absolute_error: 0.3885 - accuracy: 0.7105\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.0940 - mean_absolute_error: 0.3880 - accuracy: 0.7023\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.0924 - mean_absolute_error: 0.3853 - accuracy: 0.7087\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.0930 - mean_absolute_error: 0.3883 - accuracy: 0.7041\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.0924 - mean_absolute_error: 0.3868 - accuracy: 0.7157\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.0915 - mean_absolute_error: 0.3819 - accuracy: 0.7087\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.0921 - mean_absolute_error: 0.3858 - accuracy: 0.7058\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.0919 - mean_absolute_error: 0.3821 - accuracy: 0.7145\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.0924 - mean_absolute_error: 0.3807 - accuracy: 0.7151\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.0909 - mean_absolute_error: 0.3810 - accuracy: 0.7105\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.0917 - mean_absolute_error: 0.3805 - accuracy: 0.7180\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.0922 - mean_absolute_error: 0.3868 - accuracy: 0.7052\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.0899 - mean_absolute_error: 0.3771 - accuracy: 0.7238\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.0923 - mean_absolute_error: 0.3792 - accuracy: 0.7070\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.0902 - mean_absolute_error: 0.3788 - accuracy: 0.7192\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.0914 - mean_absolute_error: 0.3829 - accuracy: 0.7198\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.0917 - mean_absolute_error: 0.3816 - accuracy: 0.7122\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.0917 - mean_absolute_error: 0.3808 - accuracy: 0.7093\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.0907 - mean_absolute_error: 0.3819 - accuracy: 0.7134\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.0903 - mean_absolute_error: 0.3750 - accuracy: 0.7169\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.0926 - mean_absolute_error: 0.3857 - accuracy: 0.7017\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.0901 - mean_absolute_error: 0.3796 - accuracy: 0.7238\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.0914 - mean_absolute_error: 0.3776 - accuracy: 0.7145\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.0917 - mean_absolute_error: 0.3825 - accuracy: 0.7157\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.0913 - mean_absolute_error: 0.3816 - accuracy: 0.7128\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.0898 - mean_absolute_error: 0.3775 - accuracy: 0.7169\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.0919 - mean_absolute_error: 0.3784 - accuracy: 0.7105\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.0910 - mean_absolute_error: 0.3828 - accuracy: 0.7105\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.0902 - mean_absolute_error: 0.3788 - accuracy: 0.7203\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.0929 - mean_absolute_error: 0.3837 - accuracy: 0.7017\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.0940 - mean_absolute_error: 0.3895 - accuracy: 0.7128\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.0907 - mean_absolute_error: 0.3766 - accuracy: 0.7110\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.0901 - mean_absolute_error: 0.3772 - accuracy: 0.7203\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.0910 - mean_absolute_error: 0.3801 - accuracy: 0.7192\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.0896 - mean_absolute_error: 0.3788 - accuracy: 0.7186\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.0913 - mean_absolute_error: 0.3756 - accuracy: 0.7221\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.0907 - mean_absolute_error: 0.3786 - accuracy: 0.7209\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.0915 - mean_absolute_error: 0.3746 - accuracy: 0.7157\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.0912 - mean_absolute_error: 0.3833 - accuracy: 0.7110\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.0924 - mean_absolute_error: 0.3809 - accuracy: 0.7198\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.0908 - mean_absolute_error: 0.3788 - accuracy: 0.7140\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.0893 - mean_absolute_error: 0.3743 - accuracy: 0.7128\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.0924 - mean_absolute_error: 0.3833 - accuracy: 0.7093\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.0899 - mean_absolute_error: 0.3769 - accuracy: 0.7186\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.0892 - mean_absolute_error: 0.3712 - accuracy: 0.7192\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.0901 - mean_absolute_error: 0.3792 - accuracy: 0.7203\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.0894 - mean_absolute_error: 0.3742 - accuracy: 0.7198\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.0928 - mean_absolute_error: 0.3845 - accuracy: 0.7081\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.0910 - mean_absolute_error: 0.3787 - accuracy: 0.7163\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.0893 - mean_absolute_error: 0.3757 - accuracy: 0.7233\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.0882 - mean_absolute_error: 0.3672 - accuracy: 0.7279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      " - 0s - loss: 0.0899 - mean_absolute_error: 0.3746 - accuracy: 0.7244\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.0906 - mean_absolute_error: 0.3756 - accuracy: 0.7285\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.0905 - mean_absolute_error: 0.3768 - accuracy: 0.7151\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.0898 - mean_absolute_error: 0.3793 - accuracy: 0.7203\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.0892 - mean_absolute_error: 0.3690 - accuracy: 0.7209\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.0893 - mean_absolute_error: 0.3735 - accuracy: 0.7140\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.0892 - mean_absolute_error: 0.3731 - accuracy: 0.7262\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.0908 - mean_absolute_error: 0.3736 - accuracy: 0.7215\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.0917 - mean_absolute_error: 0.3756 - accuracy: 0.7157\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.0897 - mean_absolute_error: 0.3762 - accuracy: 0.7203\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.0892 - mean_absolute_error: 0.3715 - accuracy: 0.7198\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.0900 - mean_absolute_error: 0.3753 - accuracy: 0.7192\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.0899 - mean_absolute_error: 0.3737 - accuracy: 0.7267\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.0889 - mean_absolute_error: 0.3727 - accuracy: 0.7244\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.0894 - mean_absolute_error: 0.3755 - accuracy: 0.7221\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.0881 - mean_absolute_error: 0.3700 - accuracy: 0.7215\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.0894 - mean_absolute_error: 0.3704 - accuracy: 0.7256\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.0884 - mean_absolute_error: 0.3747 - accuracy: 0.7326\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.0905 - mean_absolute_error: 0.3743 - accuracy: 0.7203\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.0907 - mean_absolute_error: 0.3777 - accuracy: 0.7180\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.0899 - mean_absolute_error: 0.3722 - accuracy: 0.7192\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.0905 - mean_absolute_error: 0.3786 - accuracy: 0.7180\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.0900 - mean_absolute_error: 0.3746 - accuracy: 0.7180\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.0902 - mean_absolute_error: 0.3774 - accuracy: 0.7163\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.0897 - mean_absolute_error: 0.3739 - accuracy: 0.7174\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.0918 - mean_absolute_error: 0.3789 - accuracy: 0.7145\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.0904 - mean_absolute_error: 0.3759 - accuracy: 0.7192\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.0894 - mean_absolute_error: 0.3738 - accuracy: 0.7169\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.0888 - mean_absolute_error: 0.3733 - accuracy: 0.7250\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.0910 - mean_absolute_error: 0.3730 - accuracy: 0.7145\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.0908 - mean_absolute_error: 0.3827 - accuracy: 0.7099\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.0897 - mean_absolute_error: 0.3741 - accuracy: 0.7267\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.0895 - mean_absolute_error: 0.3760 - accuracy: 0.7314\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.0895 - mean_absolute_error: 0.3751 - accuracy: 0.7174\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.0900 - mean_absolute_error: 0.3772 - accuracy: 0.7169\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.0893 - mean_absolute_error: 0.3745 - accuracy: 0.7140\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.0874 - mean_absolute_error: 0.3693 - accuracy: 0.7297\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.0890 - mean_absolute_error: 0.3686 - accuracy: 0.7169\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.0902 - mean_absolute_error: 0.3759 - accuracy: 0.7221\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.0904 - mean_absolute_error: 0.3741 - accuracy: 0.7198\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.0885 - mean_absolute_error: 0.3698 - accuracy: 0.7186\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.0899 - mean_absolute_error: 0.3753 - accuracy: 0.7163\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.0900 - mean_absolute_error: 0.3710 - accuracy: 0.7122\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.0899 - mean_absolute_error: 0.3753 - accuracy: 0.7215\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.0900 - mean_absolute_error: 0.3750 - accuracy: 0.7180\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.0891 - mean_absolute_error: 0.3732 - accuracy: 0.7186\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.0903 - mean_absolute_error: 0.3758 - accuracy: 0.7174\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.0887 - mean_absolute_error: 0.3735 - accuracy: 0.7227\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.0891 - mean_absolute_error: 0.3706 - accuracy: 0.7273\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.0897 - mean_absolute_error: 0.3762 - accuracy: 0.7285\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.0902 - mean_absolute_error: 0.3803 - accuracy: 0.7070\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.0883 - mean_absolute_error: 0.3705 - accuracy: 0.7180\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.0887 - mean_absolute_error: 0.3717 - accuracy: 0.7221\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.0884 - mean_absolute_error: 0.3688 - accuracy: 0.7215\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.0893 - mean_absolute_error: 0.3728 - accuracy: 0.7186\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.0902 - mean_absolute_error: 0.3729 - accuracy: 0.7297\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.0888 - mean_absolute_error: 0.3723 - accuracy: 0.7203\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.0909 - mean_absolute_error: 0.3818 - accuracy: 0.7174\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.0879 - mean_absolute_error: 0.3625 - accuracy: 0.7244\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.0896 - mean_absolute_error: 0.3723 - accuracy: 0.7186\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.0890 - mean_absolute_error: 0.3697 - accuracy: 0.7244\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.0899 - mean_absolute_error: 0.3731 - accuracy: 0.7169\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.0879 - mean_absolute_error: 0.3705 - accuracy: 0.7203\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.0883 - mean_absolute_error: 0.3708 - accuracy: 0.7227\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.0889 - mean_absolute_error: 0.3735 - accuracy: 0.7262\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.0877 - mean_absolute_error: 0.3598 - accuracy: 0.7308\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.0889 - mean_absolute_error: 0.3710 - accuracy: 0.7145\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.0875 - mean_absolute_error: 0.3659 - accuracy: 0.7215\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.0877 - mean_absolute_error: 0.3668 - accuracy: 0.7250\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.0876 - mean_absolute_error: 0.3660 - accuracy: 0.7244\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.0883 - mean_absolute_error: 0.3704 - accuracy: 0.7215\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.0871 - mean_absolute_error: 0.3671 - accuracy: 0.7302\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.0882 - mean_absolute_error: 0.3653 - accuracy: 0.7267\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.0875 - mean_absolute_error: 0.3612 - accuracy: 0.7349\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.0879 - mean_absolute_error: 0.3679 - accuracy: 0.7279\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.0873 - mean_absolute_error: 0.3654 - accuracy: 0.7238\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.0883 - mean_absolute_error: 0.3673 - accuracy: 0.7233\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.0881 - mean_absolute_error: 0.3700 - accuracy: 0.7244\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.0875 - mean_absolute_error: 0.3656 - accuracy: 0.7302\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.0875 - mean_absolute_error: 0.3644 - accuracy: 0.7262\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.0883 - mean_absolute_error: 0.3705 - accuracy: 0.7174\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.0879 - mean_absolute_error: 0.3652 - accuracy: 0.7250\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.0900 - mean_absolute_error: 0.3726 - accuracy: 0.7140\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.0876 - mean_absolute_error: 0.3641 - accuracy: 0.7174\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.0889 - mean_absolute_error: 0.3730 - accuracy: 0.7233\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.0885 - mean_absolute_error: 0.3654 - accuracy: 0.7291\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.0894 - mean_absolute_error: 0.3718 - accuracy: 0.7169\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.0887 - mean_absolute_error: 0.3691 - accuracy: 0.7163\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.0894 - mean_absolute_error: 0.3715 - accuracy: 0.7157\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.0874 - mean_absolute_error: 0.3678 - accuracy: 0.7174\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.0877 - mean_absolute_error: 0.3719 - accuracy: 0.7186\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.0887 - mean_absolute_error: 0.3728 - accuracy: 0.7145\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.0886 - mean_absolute_error: 0.3668 - accuracy: 0.7209\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.0886 - mean_absolute_error: 0.3710 - accuracy: 0.7169\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.0901 - mean_absolute_error: 0.3742 - accuracy: 0.7174\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.0889 - mean_absolute_error: 0.3724 - accuracy: 0.7180\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.0885 - mean_absolute_error: 0.3700 - accuracy: 0.7186\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.0888 - mean_absolute_error: 0.3693 - accuracy: 0.7244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200\n",
      " - 0s - loss: 0.0893 - mean_absolute_error: 0.3744 - accuracy: 0.7192\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.0880 - mean_absolute_error: 0.3713 - accuracy: 0.7233\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.0871 - mean_absolute_error: 0.3631 - accuracy: 0.7209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27fd2389278>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(optimizer='adam', loss='logcosh',metrics=[metrics.mae, 'accuracy'])\n",
    "model2.fit(X1, y1, batch_size=32, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data :- \n",
      "\t loss: 0.11469146699616403   mean_absolute_error: 0.47265517711639404   accuracy: 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "loss, mean_absolute_error, accuracy =model2.evaluate(X2, y2, batch_size=32, verbose=2)\n",
    "print('Accuracy on Test Data :- ')\n",
    "print('\\t loss:',loss , '  mean_absolute_error:',mean_absolute_error, '  accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__After increasing number of epochs in the model, accuracy increased and loss decreased.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Hyperparameters:___<br>\n",
    "filters=128<br>\n",
    "kernel_size=3<br>\n",
    "Activation functions=  elu, softmax<br>\n",
    "loss= logcosh<br>\n",
    "optimizer= RMSprop, Adagrad<br>\n",
    "epochs=200 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. optimizers : RMSprop__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 0s - loss: 0.0876 - mean_absolute_error: 0.3621 - accuracy: 0.7169\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.0890 - mean_absolute_error: 0.3725 - accuracy: 0.7256\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.0888 - mean_absolute_error: 0.3672 - accuracy: 0.7250\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.0884 - mean_absolute_error: 0.3677 - accuracy: 0.7198\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.0874 - mean_absolute_error: 0.3665 - accuracy: 0.7209\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.0876 - mean_absolute_error: 0.3652 - accuracy: 0.7233\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.0883 - mean_absolute_error: 0.3668 - accuracy: 0.7308\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3640 - accuracy: 0.7192\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.0864 - mean_absolute_error: 0.3637 - accuracy: 0.7198\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.0881 - mean_absolute_error: 0.3665 - accuracy: 0.7238\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.0868 - mean_absolute_error: 0.3611 - accuracy: 0.7244\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.0868 - mean_absolute_error: 0.3623 - accuracy: 0.7215\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.0871 - mean_absolute_error: 0.3614 - accuracy: 0.7227\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.0878 - mean_absolute_error: 0.3636 - accuracy: 0.7250\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.0871 - mean_absolute_error: 0.3638 - accuracy: 0.7221\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.0883 - mean_absolute_error: 0.3682 - accuracy: 0.7215\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0899 - mean_absolute_error: 0.3689 - accuracy: 0.7157\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0877 - mean_absolute_error: 0.3649 - accuracy: 0.7262\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3600 - accuracy: 0.7244\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0867 - mean_absolute_error: 0.3603 - accuracy: 0.7285\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0877 - mean_absolute_error: 0.3661 - accuracy: 0.7267\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0877 - mean_absolute_error: 0.3630 - accuracy: 0.7169\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0877 - mean_absolute_error: 0.3677 - accuracy: 0.7279\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3592 - accuracy: 0.7320\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0864 - mean_absolute_error: 0.3600 - accuracy: 0.7326\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0881 - mean_absolute_error: 0.3641 - accuracy: 0.7291\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0858 - mean_absolute_error: 0.3576 - accuracy: 0.7233\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3628 - accuracy: 0.7285\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0867 - mean_absolute_error: 0.3621 - accuracy: 0.7331\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0879 - mean_absolute_error: 0.3619 - accuracy: 0.7250\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.0878 - mean_absolute_error: 0.3639 - accuracy: 0.7233\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.0880 - mean_absolute_error: 0.3665 - accuracy: 0.7262\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.0872 - mean_absolute_error: 0.3629 - accuracy: 0.7227\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.0874 - mean_absolute_error: 0.3612 - accuracy: 0.7273\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0847 - mean_absolute_error: 0.3535 - accuracy: 0.7267\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0874 - mean_absolute_error: 0.3650 - accuracy: 0.7227\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3613 - accuracy: 0.7157\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0879 - mean_absolute_error: 0.3634 - accuracy: 0.7209\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3584 - accuracy: 0.7314\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0870 - mean_absolute_error: 0.3626 - accuracy: 0.7320\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3627 - accuracy: 0.7163\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0878 - mean_absolute_error: 0.3635 - accuracy: 0.7203\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3629 - accuracy: 0.7215\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.0867 - mean_absolute_error: 0.3621 - accuracy: 0.7233\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.0872 - mean_absolute_error: 0.3625 - accuracy: 0.7285\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.0873 - mean_absolute_error: 0.3618 - accuracy: 0.7267\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.0870 - mean_absolute_error: 0.3637 - accuracy: 0.7267\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.0874 - mean_absolute_error: 0.3625 - accuracy: 0.7238\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3579 - accuracy: 0.7273\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.0854 - mean_absolute_error: 0.3569 - accuracy: 0.7285\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.0846 - mean_absolute_error: 0.3527 - accuracy: 0.7343\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.0868 - mean_absolute_error: 0.3639 - accuracy: 0.7343\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.0867 - mean_absolute_error: 0.3603 - accuracy: 0.7349\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.0874 - mean_absolute_error: 0.3633 - accuracy: 0.7238\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.0870 - mean_absolute_error: 0.3641 - accuracy: 0.7267\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3586 - accuracy: 0.7349\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.0882 - mean_absolute_error: 0.3652 - accuracy: 0.7273\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.0877 - mean_absolute_error: 0.3649 - accuracy: 0.7285\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.0865 - mean_absolute_error: 0.3588 - accuracy: 0.7244\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3585 - accuracy: 0.7279\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.0859 - mean_absolute_error: 0.3589 - accuracy: 0.7244\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.0888 - mean_absolute_error: 0.3656 - accuracy: 0.7169\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.0873 - mean_absolute_error: 0.3639 - accuracy: 0.7314\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.0858 - mean_absolute_error: 0.3576 - accuracy: 0.7331\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.0856 - mean_absolute_error: 0.3556 - accuracy: 0.7297\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.0871 - mean_absolute_error: 0.3600 - accuracy: 0.7314\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3607 - accuracy: 0.7203\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3600 - accuracy: 0.7285\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.0867 - mean_absolute_error: 0.3590 - accuracy: 0.7279\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.0851 - mean_absolute_error: 0.3553 - accuracy: 0.7291\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.0853 - mean_absolute_error: 0.3564 - accuracy: 0.7285\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.0871 - mean_absolute_error: 0.3623 - accuracy: 0.7256\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.0858 - mean_absolute_error: 0.3585 - accuracy: 0.7384\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3581 - accuracy: 0.7227\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3590 - accuracy: 0.7233\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3600 - accuracy: 0.7390\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.0864 - mean_absolute_error: 0.3591 - accuracy: 0.7198\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.0868 - mean_absolute_error: 0.3624 - accuracy: 0.7267\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.0852 - mean_absolute_error: 0.3582 - accuracy: 0.7366\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.0874 - mean_absolute_error: 0.3605 - accuracy: 0.7285\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.0867 - mean_absolute_error: 0.3640 - accuracy: 0.7244\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3591 - accuracy: 0.7256\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.0879 - mean_absolute_error: 0.3640 - accuracy: 0.7273\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.0880 - mean_absolute_error: 0.3640 - accuracy: 0.7326\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.0865 - mean_absolute_error: 0.3593 - accuracy: 0.7267\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.0864 - mean_absolute_error: 0.3585 - accuracy: 0.7262\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.0870 - mean_absolute_error: 0.3637 - accuracy: 0.7250\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3581 - accuracy: 0.7331\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.0859 - mean_absolute_error: 0.3567 - accuracy: 0.7297\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.0873 - mean_absolute_error: 0.3642 - accuracy: 0.7163\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.0859 - mean_absolute_error: 0.3575 - accuracy: 0.7238\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3620 - accuracy: 0.7302\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.0876 - mean_absolute_error: 0.3639 - accuracy: 0.7291\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.0873 - mean_absolute_error: 0.3603 - accuracy: 0.7355\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.0847 - mean_absolute_error: 0.3538 - accuracy: 0.7250\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3573 - accuracy: 0.7238\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.0863 - mean_absolute_error: 0.3547 - accuracy: 0.7360\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3605 - accuracy: 0.7314\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3564 - accuracy: 0.7326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      " - 0s - loss: 0.0864 - mean_absolute_error: 0.3591 - accuracy: 0.7366\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3610 - accuracy: 0.7302\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.0876 - mean_absolute_error: 0.3653 - accuracy: 0.7209\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.0864 - mean_absolute_error: 0.3603 - accuracy: 0.7134\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3615 - accuracy: 0.7279\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.0855 - mean_absolute_error: 0.3588 - accuracy: 0.7308\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.0855 - mean_absolute_error: 0.3589 - accuracy: 0.7349\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.0871 - mean_absolute_error: 0.3596 - accuracy: 0.7291\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.0840 - mean_absolute_error: 0.3538 - accuracy: 0.7331\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.0868 - mean_absolute_error: 0.3582 - accuracy: 0.7221\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.0859 - mean_absolute_error: 0.3553 - accuracy: 0.7221\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.0875 - mean_absolute_error: 0.3642 - accuracy: 0.7337\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.0852 - mean_absolute_error: 0.3546 - accuracy: 0.7314\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.0863 - mean_absolute_error: 0.3614 - accuracy: 0.7279\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.0859 - mean_absolute_error: 0.3584 - accuracy: 0.7297\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3620 - accuracy: 0.7262\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3595 - accuracy: 0.7273\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.0841 - mean_absolute_error: 0.3538 - accuracy: 0.7390\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.0874 - mean_absolute_error: 0.3590 - accuracy: 0.7343\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.0847 - mean_absolute_error: 0.3520 - accuracy: 0.7308\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.0854 - mean_absolute_error: 0.3550 - accuracy: 0.7337\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.0873 - mean_absolute_error: 0.3572 - accuracy: 0.7262\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.0856 - mean_absolute_error: 0.3585 - accuracy: 0.7285\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3612 - accuracy: 0.7320\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.0860 - mean_absolute_error: 0.3582 - accuracy: 0.7308\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3616 - accuracy: 0.7378\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.0858 - mean_absolute_error: 0.3611 - accuracy: 0.7360\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3611 - accuracy: 0.7413\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3595 - accuracy: 0.7355\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3606 - accuracy: 0.7238\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.0855 - mean_absolute_error: 0.3572 - accuracy: 0.7262\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.0849 - mean_absolute_error: 0.3539 - accuracy: 0.7343\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3598 - accuracy: 0.7209\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.0856 - mean_absolute_error: 0.3597 - accuracy: 0.7285\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3614 - accuracy: 0.7198\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.0863 - mean_absolute_error: 0.3580 - accuracy: 0.7279\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.0853 - mean_absolute_error: 0.3557 - accuracy: 0.7308\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.0864 - mean_absolute_error: 0.3596 - accuracy: 0.7297\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3597 - accuracy: 0.7308\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3579 - accuracy: 0.7285\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.0870 - mean_absolute_error: 0.3590 - accuracy: 0.7297\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3581 - accuracy: 0.7308\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.0845 - mean_absolute_error: 0.3540 - accuracy: 0.7343\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.0875 - mean_absolute_error: 0.3620 - accuracy: 0.7227\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.0838 - mean_absolute_error: 0.3551 - accuracy: 0.7401\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.0875 - mean_absolute_error: 0.3620 - accuracy: 0.7198\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.0880 - mean_absolute_error: 0.3622 - accuracy: 0.7227\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3565 - accuracy: 0.7244\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3578 - accuracy: 0.7320\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.0875 - mean_absolute_error: 0.3645 - accuracy: 0.7320\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.0873 - mean_absolute_error: 0.3621 - accuracy: 0.7221\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3608 - accuracy: 0.7343\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3619 - accuracy: 0.7378\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.0851 - mean_absolute_error: 0.3588 - accuracy: 0.7326\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.0863 - mean_absolute_error: 0.3601 - accuracy: 0.7250\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.0863 - mean_absolute_error: 0.3586 - accuracy: 0.7267\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3575 - accuracy: 0.7273\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3569 - accuracy: 0.7273\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3608 - accuracy: 0.7314\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.0852 - mean_absolute_error: 0.3575 - accuracy: 0.7355\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.0857 - mean_absolute_error: 0.3561 - accuracy: 0.7273\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.0855 - mean_absolute_error: 0.3552 - accuracy: 0.7424\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3592 - accuracy: 0.7279\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.0846 - mean_absolute_error: 0.3523 - accuracy: 0.7384\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3571 - accuracy: 0.7285\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3594 - accuracy: 0.7279\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.0837 - mean_absolute_error: 0.3495 - accuracy: 0.7413\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.0840 - mean_absolute_error: 0.3530 - accuracy: 0.7343\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.0873 - mean_absolute_error: 0.3604 - accuracy: 0.7227\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.0848 - mean_absolute_error: 0.3519 - accuracy: 0.7291\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.0842 - mean_absolute_error: 0.3532 - accuracy: 0.7343\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3564 - accuracy: 0.7238\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.0848 - mean_absolute_error: 0.3558 - accuracy: 0.7366\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3574 - accuracy: 0.7256\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.0870 - mean_absolute_error: 0.3557 - accuracy: 0.7366\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.0860 - mean_absolute_error: 0.3584 - accuracy: 0.7297\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.0851 - mean_absolute_error: 0.3550 - accuracy: 0.7320\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.0855 - mean_absolute_error: 0.3579 - accuracy: 0.7326\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.0860 - mean_absolute_error: 0.3574 - accuracy: 0.7314\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.0849 - mean_absolute_error: 0.3531 - accuracy: 0.7459\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.0835 - mean_absolute_error: 0.3496 - accuracy: 0.7407\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3568 - accuracy: 0.7331\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3563 - accuracy: 0.7407\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.0843 - mean_absolute_error: 0.3522 - accuracy: 0.7430\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3567 - accuracy: 0.7273\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.0844 - mean_absolute_error: 0.3537 - accuracy: 0.7349\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.0847 - mean_absolute_error: 0.3549 - accuracy: 0.7349\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.0849 - mean_absolute_error: 0.3530 - accuracy: 0.7384\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.0869 - mean_absolute_error: 0.3631 - accuracy: 0.7203\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.0854 - mean_absolute_error: 0.3586 - accuracy: 0.7297\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.0858 - mean_absolute_error: 0.3564 - accuracy: 0.7378\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.0840 - mean_absolute_error: 0.3507 - accuracy: 0.7331\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.0843 - mean_absolute_error: 0.3534 - accuracy: 0.7407\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.0866 - mean_absolute_error: 0.3614 - accuracy: 0.7233\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3581 - accuracy: 0.7262\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.0851 - mean_absolute_error: 0.3500 - accuracy: 0.7285\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.0861 - mean_absolute_error: 0.3574 - accuracy: 0.7250\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.0831 - mean_absolute_error: 0.3504 - accuracy: 0.7360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200\n",
      " - 0s - loss: 0.0862 - mean_absolute_error: 0.3597 - accuracy: 0.7308\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.0854 - mean_absolute_error: 0.3580 - accuracy: 0.7262\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.0850 - mean_absolute_error: 0.3553 - accuracy: 0.7395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27fd24e1eb8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSprop=optimizers.RMSprop(learning_rate=0.4, rho=0.9)\n",
    "model2.compile(optimizer='RMSprop', loss='logcosh',metrics=[metrics.mae, 'accuracy'])\n",
    "model2.fit(X1, y1, batch_size=32, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Accuracy :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data :- \n",
      "\t loss: 0.10839736822879675   mean_absolute_error: 0.45831698179244995   accuracy: 0.6969696879386902\n"
     ]
    }
   ],
   "source": [
    "loss, mean_absolute_error, accuracy =model2.evaluate(X2, y2, batch_size=32, verbose=2)\n",
    "print('Accuracy on Test Data :- ')\n",
    "print('\\t loss:',loss , '  mean_absolute_error:',mean_absolute_error, '  accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. optimizers : Adagrad__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 0s - loss: 0.0932 - mean_absolute_error: 0.3764 - accuracy: 0.7099\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.0882 - mean_absolute_error: 0.3672 - accuracy: 0.7227\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.0882 - mean_absolute_error: 0.3655 - accuracy: 0.7227\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.0843 - mean_absolute_error: 0.3586 - accuracy: 0.7407\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.0863 - mean_absolute_error: 0.3626 - accuracy: 0.7279\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.0870 - mean_absolute_error: 0.3641 - accuracy: 0.7180\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.0875 - mean_absolute_error: 0.3653 - accuracy: 0.7297\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.0858 - mean_absolute_error: 0.3620 - accuracy: 0.7302\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.0849 - mean_absolute_error: 0.3609 - accuracy: 0.7331\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.0853 - mean_absolute_error: 0.3575 - accuracy: 0.7297\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.0836 - mean_absolute_error: 0.3537 - accuracy: 0.7413\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.0846 - mean_absolute_error: 0.3546 - accuracy: 0.7320\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.0840 - mean_absolute_error: 0.3557 - accuracy: 0.7378\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.0856 - mean_absolute_error: 0.3582 - accuracy: 0.7366\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.0854 - mean_absolute_error: 0.3619 - accuracy: 0.7267\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.0850 - mean_absolute_error: 0.3608 - accuracy: 0.7279\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.0852 - mean_absolute_error: 0.3581 - accuracy: 0.7244\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.0833 - mean_absolute_error: 0.3528 - accuracy: 0.7413\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.0823 - mean_absolute_error: 0.3514 - accuracy: 0.7372\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.0852 - mean_absolute_error: 0.3584 - accuracy: 0.7291\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.0815 - mean_absolute_error: 0.3489 - accuracy: 0.7500\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.0844 - mean_absolute_error: 0.3535 - accuracy: 0.7355\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.0838 - mean_absolute_error: 0.3565 - accuracy: 0.7390\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.0837 - mean_absolute_error: 0.3525 - accuracy: 0.7297\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.0851 - mean_absolute_error: 0.3580 - accuracy: 0.7273\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.0840 - mean_absolute_error: 0.3540 - accuracy: 0.7430\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.0847 - mean_absolute_error: 0.3544 - accuracy: 0.7331\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.0822 - mean_absolute_error: 0.3501 - accuracy: 0.7477\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.0833 - mean_absolute_error: 0.3522 - accuracy: 0.7372\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.0828 - mean_absolute_error: 0.3529 - accuracy: 0.7390\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.0831 - mean_absolute_error: 0.3525 - accuracy: 0.7448\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.0833 - mean_absolute_error: 0.3506 - accuracy: 0.7360\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.0837 - mean_absolute_error: 0.3513 - accuracy: 0.7384\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.0844 - mean_absolute_error: 0.3548 - accuracy: 0.7372\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.0836 - mean_absolute_error: 0.3547 - accuracy: 0.7343\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.0843 - mean_absolute_error: 0.3564 - accuracy: 0.7384\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.0836 - mean_absolute_error: 0.3528 - accuracy: 0.7337\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.0809 - mean_absolute_error: 0.3438 - accuracy: 0.7378\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.0842 - mean_absolute_error: 0.3526 - accuracy: 0.7314\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.0843 - mean_absolute_error: 0.3555 - accuracy: 0.7279\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.0847 - mean_absolute_error: 0.3568 - accuracy: 0.7314\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.0834 - mean_absolute_error: 0.3512 - accuracy: 0.7366\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.0833 - mean_absolute_error: 0.3536 - accuracy: 0.7395\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.0823 - mean_absolute_error: 0.3484 - accuracy: 0.7413\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.0826 - mean_absolute_error: 0.3493 - accuracy: 0.7401\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.0826 - mean_absolute_error: 0.3476 - accuracy: 0.7401\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.0834 - mean_absolute_error: 0.3512 - accuracy: 0.7436\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.0822 - mean_absolute_error: 0.3488 - accuracy: 0.7419\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.0822 - mean_absolute_error: 0.3468 - accuracy: 0.7401\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.0837 - mean_absolute_error: 0.3516 - accuracy: 0.7343\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.0831 - mean_absolute_error: 0.3508 - accuracy: 0.7320\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.0839 - mean_absolute_error: 0.3535 - accuracy: 0.7413\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.0830 - mean_absolute_error: 0.3524 - accuracy: 0.7378\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.0836 - mean_absolute_error: 0.3516 - accuracy: 0.7349\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.0842 - mean_absolute_error: 0.3518 - accuracy: 0.7314\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.0840 - mean_absolute_error: 0.3528 - accuracy: 0.7395\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3480 - accuracy: 0.7424\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.0841 - mean_absolute_error: 0.3539 - accuracy: 0.7285\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.0846 - mean_absolute_error: 0.3528 - accuracy: 0.7308\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.0839 - mean_absolute_error: 0.3539 - accuracy: 0.7378\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.0817 - mean_absolute_error: 0.3481 - accuracy: 0.7442\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.0824 - mean_absolute_error: 0.3495 - accuracy: 0.7430\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.0826 - mean_absolute_error: 0.3508 - accuracy: 0.7326\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.0825 - mean_absolute_error: 0.3461 - accuracy: 0.7395\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.0828 - mean_absolute_error: 0.3491 - accuracy: 0.7395\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3469 - accuracy: 0.7436\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.0807 - mean_absolute_error: 0.3438 - accuracy: 0.7424\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.0834 - mean_absolute_error: 0.3506 - accuracy: 0.7250\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.0829 - mean_absolute_error: 0.3492 - accuracy: 0.7360\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.0820 - mean_absolute_error: 0.3490 - accuracy: 0.7477\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.0808 - mean_absolute_error: 0.3420 - accuracy: 0.7459\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3457 - accuracy: 0.7419\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.0834 - mean_absolute_error: 0.3515 - accuracy: 0.7349\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.0823 - mean_absolute_error: 0.3487 - accuracy: 0.7436\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.0820 - mean_absolute_error: 0.3460 - accuracy: 0.7419\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.0818 - mean_absolute_error: 0.3460 - accuracy: 0.7512\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.0824 - mean_absolute_error: 0.3495 - accuracy: 0.7459\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.0820 - mean_absolute_error: 0.3491 - accuracy: 0.7448\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.0823 - mean_absolute_error: 0.3488 - accuracy: 0.7424\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.0830 - mean_absolute_error: 0.3497 - accuracy: 0.7355\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.0820 - mean_absolute_error: 0.3462 - accuracy: 0.7442\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.0826 - mean_absolute_error: 0.3460 - accuracy: 0.7436\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.0825 - mean_absolute_error: 0.3480 - accuracy: 0.7477\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.0833 - mean_absolute_error: 0.3510 - accuracy: 0.7413\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.0806 - mean_absolute_error: 0.3428 - accuracy: 0.7483\n",
      "Epoch 86/200\n",
      " - 0s - loss: 0.0827 - mean_absolute_error: 0.3484 - accuracy: 0.7471\n",
      "Epoch 87/200\n",
      " - 0s - loss: 0.0848 - mean_absolute_error: 0.3536 - accuracy: 0.7407\n",
      "Epoch 88/200\n",
      " - 0s - loss: 0.0805 - mean_absolute_error: 0.3438 - accuracy: 0.7442\n",
      "Epoch 89/200\n",
      " - 0s - loss: 0.0827 - mean_absolute_error: 0.3475 - accuracy: 0.7390\n",
      "Epoch 90/200\n",
      " - 0s - loss: 0.0808 - mean_absolute_error: 0.3441 - accuracy: 0.7477\n",
      "Epoch 91/200\n",
      " - 0s - loss: 0.0818 - mean_absolute_error: 0.3472 - accuracy: 0.7442\n",
      "Epoch 92/200\n",
      " - 0s - loss: 0.0830 - mean_absolute_error: 0.3515 - accuracy: 0.7453\n",
      "Epoch 93/200\n",
      " - 0s - loss: 0.0822 - mean_absolute_error: 0.3471 - accuracy: 0.7436\n",
      "Epoch 94/200\n",
      " - 0s - loss: 0.0820 - mean_absolute_error: 0.3478 - accuracy: 0.7419\n",
      "Epoch 95/200\n",
      " - 0s - loss: 0.0846 - mean_absolute_error: 0.3527 - accuracy: 0.7320\n",
      "Epoch 96/200\n",
      " - 0s - loss: 0.0811 - mean_absolute_error: 0.3444 - accuracy: 0.7459\n",
      "Epoch 97/200\n",
      " - 0s - loss: 0.0818 - mean_absolute_error: 0.3461 - accuracy: 0.7407\n",
      "Epoch 98/200\n",
      " - 0s - loss: 0.0827 - mean_absolute_error: 0.3504 - accuracy: 0.7442\n",
      "Epoch 99/200\n",
      " - 0s - loss: 0.0820 - mean_absolute_error: 0.3484 - accuracy: 0.7436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      " - 0s - loss: 0.0830 - mean_absolute_error: 0.3474 - accuracy: 0.7355\n",
      "Epoch 101/200\n",
      " - 0s - loss: 0.0810 - mean_absolute_error: 0.3455 - accuracy: 0.7407\n",
      "Epoch 102/200\n",
      " - 0s - loss: 0.0816 - mean_absolute_error: 0.3460 - accuracy: 0.7459\n",
      "Epoch 103/200\n",
      " - 0s - loss: 0.0825 - mean_absolute_error: 0.3472 - accuracy: 0.7372\n",
      "Epoch 104/200\n",
      " - 0s - loss: 0.0824 - mean_absolute_error: 0.3451 - accuracy: 0.7401\n",
      "Epoch 105/200\n",
      " - 0s - loss: 0.0810 - mean_absolute_error: 0.3455 - accuracy: 0.7506\n",
      "Epoch 106/200\n",
      " - 0s - loss: 0.0810 - mean_absolute_error: 0.3446 - accuracy: 0.7494\n",
      "Epoch 107/200\n",
      " - 0s - loss: 0.0827 - mean_absolute_error: 0.3485 - accuracy: 0.7378\n",
      "Epoch 108/200\n",
      " - 0s - loss: 0.0820 - mean_absolute_error: 0.3459 - accuracy: 0.7488\n",
      "Epoch 109/200\n",
      " - 0s - loss: 0.0811 - mean_absolute_error: 0.3443 - accuracy: 0.7465\n",
      "Epoch 110/200\n",
      " - 0s - loss: 0.0838 - mean_absolute_error: 0.3487 - accuracy: 0.7355\n",
      "Epoch 111/200\n",
      " - 0s - loss: 0.0817 - mean_absolute_error: 0.3471 - accuracy: 0.7436\n",
      "Epoch 112/200\n",
      " - 0s - loss: 0.0826 - mean_absolute_error: 0.3486 - accuracy: 0.7430\n",
      "Epoch 113/200\n",
      " - 0s - loss: 0.0813 - mean_absolute_error: 0.3458 - accuracy: 0.7471\n",
      "Epoch 114/200\n",
      " - 0s - loss: 0.0817 - mean_absolute_error: 0.3448 - accuracy: 0.7523\n",
      "Epoch 115/200\n",
      " - 0s - loss: 0.0797 - mean_absolute_error: 0.3426 - accuracy: 0.7430\n",
      "Epoch 116/200\n",
      " - 0s - loss: 0.0811 - mean_absolute_error: 0.3421 - accuracy: 0.7407\n",
      "Epoch 117/200\n",
      " - 0s - loss: 0.0825 - mean_absolute_error: 0.3452 - accuracy: 0.7419\n",
      "Epoch 118/200\n",
      " - 0s - loss: 0.0814 - mean_absolute_error: 0.3429 - accuracy: 0.7483\n",
      "Epoch 119/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3456 - accuracy: 0.7500\n",
      "Epoch 120/200\n",
      " - 0s - loss: 0.0823 - mean_absolute_error: 0.3485 - accuracy: 0.7570\n",
      "Epoch 121/200\n",
      " - 0s - loss: 0.0817 - mean_absolute_error: 0.3457 - accuracy: 0.7331\n",
      "Epoch 122/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3480 - accuracy: 0.7372\n",
      "Epoch 123/200\n",
      " - 0s - loss: 0.0824 - mean_absolute_error: 0.3483 - accuracy: 0.7465\n",
      "Epoch 124/200\n",
      " - 0s - loss: 0.0810 - mean_absolute_error: 0.3450 - accuracy: 0.7448\n",
      "Epoch 125/200\n",
      " - 0s - loss: 0.0812 - mean_absolute_error: 0.3451 - accuracy: 0.7413\n",
      "Epoch 126/200\n",
      " - 0s - loss: 0.0823 - mean_absolute_error: 0.3469 - accuracy: 0.7424\n",
      "Epoch 127/200\n",
      " - 0s - loss: 0.0828 - mean_absolute_error: 0.3462 - accuracy: 0.7471\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.0817 - mean_absolute_error: 0.3435 - accuracy: 0.7459\n",
      "Epoch 129/200\n",
      " - 0s - loss: 0.0823 - mean_absolute_error: 0.3467 - accuracy: 0.7343\n",
      "Epoch 130/200\n",
      " - 0s - loss: 0.0821 - mean_absolute_error: 0.3475 - accuracy: 0.7401\n",
      "Epoch 131/200\n",
      " - 0s - loss: 0.0825 - mean_absolute_error: 0.3471 - accuracy: 0.7413\n",
      "Epoch 132/200\n",
      " - 0s - loss: 0.0814 - mean_absolute_error: 0.3450 - accuracy: 0.7430\n",
      "Epoch 133/200\n",
      " - 0s - loss: 0.0807 - mean_absolute_error: 0.3437 - accuracy: 0.7506\n",
      "Epoch 134/200\n",
      " - 0s - loss: 0.0836 - mean_absolute_error: 0.3494 - accuracy: 0.7337\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.0811 - mean_absolute_error: 0.3451 - accuracy: 0.7378\n",
      "Epoch 136/200\n",
      " - 0s - loss: 0.0805 - mean_absolute_error: 0.3414 - accuracy: 0.7483\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.0804 - mean_absolute_error: 0.3430 - accuracy: 0.7459\n",
      "Epoch 138/200\n",
      " - 0s - loss: 0.0825 - mean_absolute_error: 0.3482 - accuracy: 0.7477\n",
      "Epoch 139/200\n",
      " - 0s - loss: 0.0821 - mean_absolute_error: 0.3462 - accuracy: 0.7488\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.0811 - mean_absolute_error: 0.3438 - accuracy: 0.7448\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.0814 - mean_absolute_error: 0.3427 - accuracy: 0.7477\n",
      "Epoch 142/200\n",
      " - 0s - loss: 0.0817 - mean_absolute_error: 0.3441 - accuracy: 0.7355\n",
      "Epoch 143/200\n",
      " - 0s - loss: 0.0815 - mean_absolute_error: 0.3447 - accuracy: 0.7419\n",
      "Epoch 144/200\n",
      " - 0s - loss: 0.0811 - mean_absolute_error: 0.3427 - accuracy: 0.7523\n",
      "Epoch 145/200\n",
      " - 0s - loss: 0.0817 - mean_absolute_error: 0.3440 - accuracy: 0.7395\n",
      "Epoch 146/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3452 - accuracy: 0.7407\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.0802 - mean_absolute_error: 0.3412 - accuracy: 0.7407\n",
      "Epoch 148/200\n",
      " - 0s - loss: 0.0791 - mean_absolute_error: 0.3372 - accuracy: 0.7622\n",
      "Epoch 149/200\n",
      " - 0s - loss: 0.0821 - mean_absolute_error: 0.3458 - accuracy: 0.7488\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.0811 - mean_absolute_error: 0.3438 - accuracy: 0.7430\n",
      "Epoch 151/200\n",
      " - 0s - loss: 0.0821 - mean_absolute_error: 0.3450 - accuracy: 0.7372\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.0816 - mean_absolute_error: 0.3465 - accuracy: 0.7378\n",
      "Epoch 153/200\n",
      " - 0s - loss: 0.0826 - mean_absolute_error: 0.3489 - accuracy: 0.7384\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.0816 - mean_absolute_error: 0.3457 - accuracy: 0.7419\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.0824 - mean_absolute_error: 0.3475 - accuracy: 0.7337\n",
      "Epoch 156/200\n",
      " - 0s - loss: 0.0821 - mean_absolute_error: 0.3466 - accuracy: 0.7453\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.0817 - mean_absolute_error: 0.3441 - accuracy: 0.7494\n",
      "Epoch 158/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3468 - accuracy: 0.7384\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.0811 - mean_absolute_error: 0.3444 - accuracy: 0.7483\n",
      "Epoch 160/200\n",
      " - 0s - loss: 0.0820 - mean_absolute_error: 0.3469 - accuracy: 0.7343\n",
      "Epoch 161/200\n",
      " - 0s - loss: 0.0815 - mean_absolute_error: 0.3466 - accuracy: 0.7419\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.0814 - mean_absolute_error: 0.3453 - accuracy: 0.7424\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.0829 - mean_absolute_error: 0.3481 - accuracy: 0.7424\n",
      "Epoch 164/200\n",
      " - 0s - loss: 0.0814 - mean_absolute_error: 0.3460 - accuracy: 0.7442\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.0816 - mean_absolute_error: 0.3475 - accuracy: 0.7442\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3469 - accuracy: 0.7390\n",
      "Epoch 167/200\n",
      " - 0s - loss: 0.0826 - mean_absolute_error: 0.3464 - accuracy: 0.7424\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.0802 - mean_absolute_error: 0.3416 - accuracy: 0.7459\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.0801 - mean_absolute_error: 0.3401 - accuracy: 0.7517\n",
      "Epoch 170/200\n",
      " - 0s - loss: 0.0814 - mean_absolute_error: 0.3451 - accuracy: 0.7465\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.0828 - mean_absolute_error: 0.3477 - accuracy: 0.7337\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.0810 - mean_absolute_error: 0.3452 - accuracy: 0.7512\n",
      "Epoch 173/200\n",
      " - 0s - loss: 0.0803 - mean_absolute_error: 0.3406 - accuracy: 0.7424\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.0805 - mean_absolute_error: 0.3434 - accuracy: 0.7378\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.0811 - mean_absolute_error: 0.3435 - accuracy: 0.7535\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.0817 - mean_absolute_error: 0.3475 - accuracy: 0.7448\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.0808 - mean_absolute_error: 0.3429 - accuracy: 0.7541\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.0798 - mean_absolute_error: 0.3424 - accuracy: 0.7517\n",
      "Epoch 179/200\n",
      " - 0s - loss: 0.0799 - mean_absolute_error: 0.3404 - accuracy: 0.7407\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.0812 - mean_absolute_error: 0.3442 - accuracy: 0.7494\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.0810 - mean_absolute_error: 0.3433 - accuracy: 0.7471\n",
      "Epoch 182/200\n",
      " - 0s - loss: 0.0799 - mean_absolute_error: 0.3392 - accuracy: 0.7477\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.0809 - mean_absolute_error: 0.3413 - accuracy: 0.7384\n",
      "Epoch 184/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3449 - accuracy: 0.7494\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.0801 - mean_absolute_error: 0.3419 - accuracy: 0.7512\n",
      "Epoch 186/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3467 - accuracy: 0.7320\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3449 - accuracy: 0.7424\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.0800 - mean_absolute_error: 0.3405 - accuracy: 0.7506\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.0807 - mean_absolute_error: 0.3440 - accuracy: 0.7448\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.0804 - mean_absolute_error: 0.3423 - accuracy: 0.7442\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.0804 - mean_absolute_error: 0.3412 - accuracy: 0.7547\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.0818 - mean_absolute_error: 0.3435 - accuracy: 0.7430\n",
      "Epoch 193/200\n",
      " - 0s - loss: 0.0805 - mean_absolute_error: 0.3418 - accuracy: 0.7390\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.0810 - mean_absolute_error: 0.3437 - accuracy: 0.7494\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.0801 - mean_absolute_error: 0.3394 - accuracy: 0.7413\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.0798 - mean_absolute_error: 0.3410 - accuracy: 0.7547\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.0813 - mean_absolute_error: 0.3415 - accuracy: 0.7494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200\n",
      " - 0s - loss: 0.0826 - mean_absolute_error: 0.3470 - accuracy: 0.7384\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.0807 - mean_absolute_error: 0.3456 - accuracy: 0.7576\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.0819 - mean_absolute_error: 0.3453 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27fdde90fd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adagrad=optimizers.Adagrad(learning_rate=0.001)\n",
    "model2.compile(optimizer='Adagrad', loss='logcosh',metrics=[metrics.mae, 'accuracy'])\n",
    "model2.fit(X1, y1, batch_size=32, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data :- \n",
      "\t loss: 0.10720591111616655   mean_absolute_error: 0.462567538022995   accuracy: 0.6969696879386902\n"
     ]
    }
   ],
   "source": [
    "loss, mean_absolute_error, accuracy =model2.evaluate(X2, y2, batch_size=32, verbose=2)\n",
    "print('Accuracy on Test Data :- ')\n",
    "print('\\t loss:',loss , '  mean_absolute_error:',mean_absolute_error, '  accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimizer: Adagrad is better suit for this model than RMSprop. Although, both are close enough in terms of accuracy and loss.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Hyperparameters:___<br>\n",
    "filters=256<br>\n",
    "kernel_size=2<br>\n",
    "Activation functions=  relu, elu, softmax<br>\n",
    "loss=logcosh<br>\n",
    "optimizer= adagrad<br>\n",
    "epoch=100<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv1D(filters=256, kernel_size=3, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(BatchNormalization())\n",
    "\n",
    "model3.add(Dense(128, activation='elu'))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv1D(filters=128, kernel_size=2, activation='elu'))\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Dense(64))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Dense(20, activation='softmax'))\n",
    "model3.add(Dropout(0.25))\n",
    "model3.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 0.1323 - mean_absolute_error: 0.4539 - accuracy: 0.6233\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1209 - mean_absolute_error: 0.4423 - accuracy: 0.6233\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1179 - mean_absolute_error: 0.4373 - accuracy: 0.6203\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1130 - mean_absolute_error: 0.4227 - accuracy: 0.6459\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1164 - mean_absolute_error: 0.4331 - accuracy: 0.6238\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.1142 - mean_absolute_error: 0.4335 - accuracy: 0.6349\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.1151 - mean_absolute_error: 0.4348 - accuracy: 0.6267\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.1150 - mean_absolute_error: 0.4368 - accuracy: 0.6267\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.1118 - mean_absolute_error: 0.4339 - accuracy: 0.6302\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.1065 - mean_absolute_error: 0.4257 - accuracy: 0.6564\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.1123 - mean_absolute_error: 0.4346 - accuracy: 0.6145\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.1087 - mean_absolute_error: 0.4308 - accuracy: 0.6360\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.1073 - mean_absolute_error: 0.4305 - accuracy: 0.6390\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.1070 - mean_absolute_error: 0.4281 - accuracy: 0.6413\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.1080 - mean_absolute_error: 0.4301 - accuracy: 0.6442\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.1072 - mean_absolute_error: 0.4280 - accuracy: 0.6419\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.1068 - mean_absolute_error: 0.4266 - accuracy: 0.6419\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.1040 - mean_absolute_error: 0.4194 - accuracy: 0.6576\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.1038 - mean_absolute_error: 0.4183 - accuracy: 0.6558\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.1024 - mean_absolute_error: 0.4118 - accuracy: 0.6593\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.1050 - mean_absolute_error: 0.4187 - accuracy: 0.6593\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.1041 - mean_absolute_error: 0.4210 - accuracy: 0.6576\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.1050 - mean_absolute_error: 0.4211 - accuracy: 0.6558\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.1007 - mean_absolute_error: 0.4093 - accuracy: 0.6738\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.1009 - mean_absolute_error: 0.4087 - accuracy: 0.6686\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.1031 - mean_absolute_error: 0.4156 - accuracy: 0.6576\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.1015 - mean_absolute_error: 0.4111 - accuracy: 0.6576\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.1029 - mean_absolute_error: 0.4100 - accuracy: 0.6535\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.1041 - mean_absolute_error: 0.4155 - accuracy: 0.6477\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.1029 - mean_absolute_error: 0.4162 - accuracy: 0.6541\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.1004 - mean_absolute_error: 0.4063 - accuracy: 0.6709\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.1030 - mean_absolute_error: 0.4114 - accuracy: 0.6593\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.1018 - mean_absolute_error: 0.4086 - accuracy: 0.6581\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.1002 - mean_absolute_error: 0.4068 - accuracy: 0.6663\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0993 - mean_absolute_error: 0.4055 - accuracy: 0.6738\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.1012 - mean_absolute_error: 0.4112 - accuracy: 0.6663\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.1009 - mean_absolute_error: 0.4108 - accuracy: 0.6657\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0981 - mean_absolute_error: 0.4026 - accuracy: 0.6814\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0990 - mean_absolute_error: 0.4051 - accuracy: 0.6733\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.1007 - mean_absolute_error: 0.4096 - accuracy: 0.6645\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.1002 - mean_absolute_error: 0.4070 - accuracy: 0.6698\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0998 - mean_absolute_error: 0.4042 - accuracy: 0.6703\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.1014 - mean_absolute_error: 0.4106 - accuracy: 0.6686\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0994 - mean_absolute_error: 0.4048 - accuracy: 0.6715\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.1015 - mean_absolute_error: 0.4117 - accuracy: 0.6599\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0974 - mean_absolute_error: 0.4016 - accuracy: 0.6872\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0993 - mean_absolute_error: 0.4076 - accuracy: 0.6709\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0997 - mean_absolute_error: 0.4077 - accuracy: 0.6721\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0998 - mean_absolute_error: 0.4071 - accuracy: 0.6733\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0996 - mean_absolute_error: 0.4067 - accuracy: 0.6750\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0996 - mean_absolute_error: 0.4099 - accuracy: 0.6709\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0992 - mean_absolute_error: 0.4056 - accuracy: 0.6767\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0987 - mean_absolute_error: 0.4050 - accuracy: 0.6767\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0973 - mean_absolute_error: 0.4018 - accuracy: 0.6808\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0986 - mean_absolute_error: 0.4051 - accuracy: 0.6808\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0983 - mean_absolute_error: 0.4038 - accuracy: 0.6773\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0980 - mean_absolute_error: 0.4028 - accuracy: 0.6762\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0975 - mean_absolute_error: 0.4019 - accuracy: 0.6907\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0988 - mean_absolute_error: 0.4061 - accuracy: 0.6791\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0956 - mean_absolute_error: 0.3958 - accuracy: 0.6924\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0966 - mean_absolute_error: 0.3972 - accuracy: 0.6913\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0972 - mean_absolute_error: 0.3997 - accuracy: 0.6913\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0981 - mean_absolute_error: 0.4023 - accuracy: 0.6820\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0977 - mean_absolute_error: 0.4032 - accuracy: 0.6843\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0960 - mean_absolute_error: 0.3932 - accuracy: 0.6948\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0963 - mean_absolute_error: 0.3969 - accuracy: 0.6872\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0944 - mean_absolute_error: 0.3896 - accuracy: 0.6971\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0986 - mean_absolute_error: 0.4038 - accuracy: 0.6849\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0965 - mean_absolute_error: 0.3974 - accuracy: 0.6884\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0982 - mean_absolute_error: 0.4014 - accuracy: 0.6791\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0980 - mean_absolute_error: 0.4001 - accuracy: 0.6860\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0955 - mean_absolute_error: 0.3948 - accuracy: 0.6948\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0959 - mean_absolute_error: 0.3965 - accuracy: 0.6953\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0970 - mean_absolute_error: 0.3998 - accuracy: 0.7017\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0973 - mean_absolute_error: 0.3996 - accuracy: 0.6837\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0960 - mean_absolute_error: 0.3966 - accuracy: 0.6895\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0969 - mean_absolute_error: 0.3989 - accuracy: 0.6890\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0964 - mean_absolute_error: 0.3989 - accuracy: 0.6953\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0982 - mean_absolute_error: 0.4019 - accuracy: 0.6837\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0965 - mean_absolute_error: 0.3953 - accuracy: 0.6855\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0961 - mean_absolute_error: 0.3916 - accuracy: 0.6965\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0956 - mean_absolute_error: 0.3937 - accuracy: 0.6959\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0958 - mean_absolute_error: 0.3948 - accuracy: 0.6988\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0961 - mean_absolute_error: 0.3962 - accuracy: 0.6971\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0976 - mean_absolute_error: 0.3978 - accuracy: 0.6890\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0965 - mean_absolute_error: 0.3973 - accuracy: 0.6948\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0965 - mean_absolute_error: 0.3979 - accuracy: 0.6855\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0947 - mean_absolute_error: 0.3924 - accuracy: 0.6977\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0959 - mean_absolute_error: 0.3962 - accuracy: 0.6971\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0953 - mean_absolute_error: 0.3904 - accuracy: 0.7012\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0955 - mean_absolute_error: 0.3954 - accuracy: 0.6959\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0958 - mean_absolute_error: 0.3896 - accuracy: 0.6988\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0953 - mean_absolute_error: 0.3916 - accuracy: 0.6977\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0957 - mean_absolute_error: 0.3939 - accuracy: 0.6983\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0953 - mean_absolute_error: 0.3951 - accuracy: 0.7000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0970 - mean_absolute_error: 0.3976 - accuracy: 0.6907\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0966 - mean_absolute_error: 0.3986 - accuracy: 0.6849\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0952 - mean_absolute_error: 0.3913 - accuracy: 0.6971\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0951 - mean_absolute_error: 0.3921 - accuracy: 0.6942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      " - 0s - loss: 0.0940 - mean_absolute_error: 0.3915 - accuracy: 0.6965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x28070b6efd0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adagrad=optimizers.Adagrad(learning_rate=0.4)\n",
    "model3.compile(optimizer='Adagrad', loss='logcosh',metrics=[metrics.mae, 'accuracy'])\n",
    "model3.fit(X1, y1, batch_size=32, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data :- \n",
      "\t loss: 0.09973003647544167   mean_absolute_error: 0.4202899932861328   accuracy: 0.6969696879386902\n"
     ]
    }
   ],
   "source": [
    "loss, mean_absolute_error, accuracy =model3.evaluate(X2, y2, batch_size=32, verbose=2)\n",
    "print('Accuracy on Test Data :- ')\n",
    "print('\\t loss:',loss , '  mean_absolute_error:',mean_absolute_error, '  accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__After changing the number of layers and the size of the layers, both accuracy and loss got improve.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Hyperparameters:___<br>\n",
    "filters=256<br>\n",
    "kernel_size=3,2<br>\n",
    "Activation functions= relu, elu, softmax<br>\n",
    "kernel_initializer=Ones, RandomNormal <br>\n",
    "loss=mean_squared_error<br>\n",
    "optimizer= adam<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "# Default : kernel_initializer='glorot_uniform'\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Conv1D(filters=256, kernel_size=3, activation='relu', kernel_initializer='Ones' , input_shape=(n_steps, n_features)))\n",
    "model4.add(MaxPooling1D(pool_size=2))\n",
    "model4.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model4.add(Conv1D(filters=128, kernel_size=2, activation='elu',kernel_initializer='RandomNormal'))\n",
    "model4.add(MaxPooling1D(pool_size=2))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model4.add(Flatten())\n",
    "\n",
    "model4.add(Dropout(0.25))\n",
    "model4.add(Dense(20, activation='softmax'))\n",
    "model4.add(Dropout(0.25))\n",
    "model4.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 0.1557 - mean_absolute_error: 0.5087 - accuracy: 0.5331\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1251 - mean_absolute_error: 0.4715 - accuracy: 0.6116\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1203 - mean_absolute_error: 0.4564 - accuracy: 0.6134\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1118 - mean_absolute_error: 0.4376 - accuracy: 0.6442\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1156 - mean_absolute_error: 0.4476 - accuracy: 0.6169\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.1150 - mean_absolute_error: 0.4465 - accuracy: 0.6227\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.1136 - mean_absolute_error: 0.4402 - accuracy: 0.6279\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.1117 - mean_absolute_error: 0.4401 - accuracy: 0.6337\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.1107 - mean_absolute_error: 0.4356 - accuracy: 0.6430\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.1111 - mean_absolute_error: 0.4376 - accuracy: 0.6308\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.1123 - mean_absolute_error: 0.4390 - accuracy: 0.6320\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.1110 - mean_absolute_error: 0.4356 - accuracy: 0.6308\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.1105 - mean_absolute_error: 0.4338 - accuracy: 0.6355\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.1085 - mean_absolute_error: 0.4300 - accuracy: 0.6442\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.1103 - mean_absolute_error: 0.4307 - accuracy: 0.6343\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.1097 - mean_absolute_error: 0.4311 - accuracy: 0.6331\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.1106 - mean_absolute_error: 0.4357 - accuracy: 0.6308\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.1090 - mean_absolute_error: 0.4278 - accuracy: 0.6395\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.1122 - mean_absolute_error: 0.4399 - accuracy: 0.6256\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.1071 - mean_absolute_error: 0.4276 - accuracy: 0.6419\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.1104 - mean_absolute_error: 0.4362 - accuracy: 0.6273\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.1078 - mean_absolute_error: 0.4301 - accuracy: 0.6378\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.1087 - mean_absolute_error: 0.4324 - accuracy: 0.6390\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.1067 - mean_absolute_error: 0.4251 - accuracy: 0.6436\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.1070 - mean_absolute_error: 0.4316 - accuracy: 0.6413\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.1077 - mean_absolute_error: 0.4405 - accuracy: 0.6308\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.1063 - mean_absolute_error: 0.4344 - accuracy: 0.6302\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.1063 - mean_absolute_error: 0.4337 - accuracy: 0.6430\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.1054 - mean_absolute_error: 0.4268 - accuracy: 0.6552\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.1067 - mean_absolute_error: 0.4307 - accuracy: 0.6488\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.1085 - mean_absolute_error: 0.4399 - accuracy: 0.6349\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.1050 - mean_absolute_error: 0.4338 - accuracy: 0.6506\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.1083 - mean_absolute_error: 0.4403 - accuracy: 0.6366\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.1052 - mean_absolute_error: 0.4338 - accuracy: 0.6471\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.1052 - mean_absolute_error: 0.4323 - accuracy: 0.6500\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.1071 - mean_absolute_error: 0.4368 - accuracy: 0.6285\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.1054 - mean_absolute_error: 0.4335 - accuracy: 0.6442\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.1043 - mean_absolute_error: 0.4345 - accuracy: 0.6523\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.1051 - mean_absolute_error: 0.4358 - accuracy: 0.6436\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.1039 - mean_absolute_error: 0.4294 - accuracy: 0.6576\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.1032 - mean_absolute_error: 0.4273 - accuracy: 0.6570\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.1031 - mean_absolute_error: 0.4291 - accuracy: 0.6581\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.1034 - mean_absolute_error: 0.4275 - accuracy: 0.6517\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.1049 - mean_absolute_error: 0.4309 - accuracy: 0.6419\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.1047 - mean_absolute_error: 0.4298 - accuracy: 0.6453\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.1040 - mean_absolute_error: 0.4311 - accuracy: 0.6552\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.1030 - mean_absolute_error: 0.4271 - accuracy: 0.6564\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.1032 - mean_absolute_error: 0.4270 - accuracy: 0.6547\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.1047 - mean_absolute_error: 0.4323 - accuracy: 0.6430\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.1038 - mean_absolute_error: 0.4297 - accuracy: 0.6477\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.1016 - mean_absolute_error: 0.4216 - accuracy: 0.6634\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.1034 - mean_absolute_error: 0.4271 - accuracy: 0.6570\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.1042 - mean_absolute_error: 0.4300 - accuracy: 0.6541\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.1029 - mean_absolute_error: 0.4261 - accuracy: 0.6634\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.1033 - mean_absolute_error: 0.4285 - accuracy: 0.6552\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.1018 - mean_absolute_error: 0.4229 - accuracy: 0.6605\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.1034 - mean_absolute_error: 0.4290 - accuracy: 0.6541\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.1028 - mean_absolute_error: 0.4286 - accuracy: 0.6558\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.1039 - mean_absolute_error: 0.4289 - accuracy: 0.6570\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.1024 - mean_absolute_error: 0.4245 - accuracy: 0.6634\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.1029 - mean_absolute_error: 0.4281 - accuracy: 0.6512\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.1031 - mean_absolute_error: 0.4250 - accuracy: 0.6576\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.1033 - mean_absolute_error: 0.4240 - accuracy: 0.6442\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.1015 - mean_absolute_error: 0.4208 - accuracy: 0.6558\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.1006 - mean_absolute_error: 0.4150 - accuracy: 0.6715\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.1011 - mean_absolute_error: 0.4192 - accuracy: 0.6663\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.1018 - mean_absolute_error: 0.4204 - accuracy: 0.6628\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.1024 - mean_absolute_error: 0.4230 - accuracy: 0.6599\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.1020 - mean_absolute_error: 0.4239 - accuracy: 0.6610\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.1017 - mean_absolute_error: 0.4222 - accuracy: 0.6576\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.1028 - mean_absolute_error: 0.4221 - accuracy: 0.6616\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.1037 - mean_absolute_error: 0.4249 - accuracy: 0.6535\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.1016 - mean_absolute_error: 0.4196 - accuracy: 0.6587\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.1021 - mean_absolute_error: 0.4236 - accuracy: 0.6663\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.1029 - mean_absolute_error: 0.4249 - accuracy: 0.6552\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.1023 - mean_absolute_error: 0.4262 - accuracy: 0.6599\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.1000 - mean_absolute_error: 0.4180 - accuracy: 0.6703\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.1015 - mean_absolute_error: 0.4181 - accuracy: 0.6686\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.1005 - mean_absolute_error: 0.4189 - accuracy: 0.6674\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.1017 - mean_absolute_error: 0.4216 - accuracy: 0.6616\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.1002 - mean_absolute_error: 0.4209 - accuracy: 0.6674\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.1014 - mean_absolute_error: 0.4225 - accuracy: 0.6686\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.1017 - mean_absolute_error: 0.4242 - accuracy: 0.6663\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0999 - mean_absolute_error: 0.4182 - accuracy: 0.6738\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.1002 - mean_absolute_error: 0.4168 - accuracy: 0.6634\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.1019 - mean_absolute_error: 0.4211 - accuracy: 0.6634\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.1000 - mean_absolute_error: 0.4159 - accuracy: 0.6703\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.1004 - mean_absolute_error: 0.4190 - accuracy: 0.6686\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.1021 - mean_absolute_error: 0.4232 - accuracy: 0.6529\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.1009 - mean_absolute_error: 0.4175 - accuracy: 0.6640\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.1025 - mean_absolute_error: 0.4227 - accuracy: 0.6593\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.1023 - mean_absolute_error: 0.4218 - accuracy: 0.6605\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.1003 - mean_absolute_error: 0.4191 - accuracy: 0.6640\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.1002 - mean_absolute_error: 0.4162 - accuracy: 0.6703\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.1022 - mean_absolute_error: 0.4228 - accuracy: 0.6523\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.1013 - mean_absolute_error: 0.4163 - accuracy: 0.6703\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.1005 - mean_absolute_error: 0.4125 - accuracy: 0.6698\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0991 - mean_absolute_error: 0.4118 - accuracy: 0.6698\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.1014 - mean_absolute_error: 0.4183 - accuracy: 0.6628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      " - 0s - loss: 0.1028 - mean_absolute_error: 0.4246 - accuracy: 0.6564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27fddb21710>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Adagrad=optimizers.Adagrad(learning_rate=0.01)\n",
    "model4.compile(optimizer='Adagrad', loss='logcosh',metrics=[metrics.mae, 'accuracy'])\n",
    "model4.fit(X1, y1, batch_size=32, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data :- \n",
      "\t loss: 0.09965821287848732   mean_absolute_error: 0.41854360699653625   accuracy: 0.6969696879386902\n"
     ]
    }
   ],
   "source": [
    "loss, mean_absolute_error, accuracy =model4.evaluate(X2, y2, batch_size=32, verbose=2)\n",
    "print('Accuracy on Test Data :- ')\n",
    "print('\\t loss:',loss , '  mean_absolute_error:',mean_absolute_error, '  accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__After changing the kernel initialization, accuracy on validation set got increase.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "After performing analysis on the CNN model, it is observed that, <br>\n",
    ">1. For the baseline model, Loss is Higher and Accuracy on validation data is 63%.<br> \n",
    ">2. The cost function logcosh is appropriate for this model than hinge.<br> \n",
    ">3. Higher the number of epochs, better the model is.<br>\n",
    ">4. Change in Optimizer can increase the accuracy and it helps to reduce the loss. Adagrad is better suit for this type of model than RMSprop<br>\n",
    ">5. Network architecture is as important as hyperparameters. By adding more layers and with proper use of hyperparamters, we can achieve higher accuracy.<br>\n",
    ">6. After changing the kernel initialization, accuracy on validation set got increase.\n",
    "\n",
    "For Multivariate Tabular data, Convolution Neural Network can produce good results but handling of data For the CNN model is difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: \n",
    "Pranav Khiste( NUID : 001057866 )<br>\n",
    "Information Systems <br>\n",
    "Northeastern University <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation:\n",
    "References: <br>\n",
    "https://keras.io/models<br>\n",
    "https://machinelearningmastery.com/ <br>\n",
    "https://karpathy.github.io/2019/04/25/recipe<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Pranav Sanjay Khiste\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
